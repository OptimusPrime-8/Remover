{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea899db-c846-4991-a27b-a2c24f0a9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "29e95eb6-2437-4f4d-b87e-9ce61fe5d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyBneAP9zHGsalKlex7cohxVTs9LLAgdRt8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5c0c03-50d1-41ee-ba0f-2d27c1fff533",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prefix=prefix=\"Global Intelligent Virtual Assistant Market 2025-2029_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c995b56-d628-4aab-8b7f-a4d66ac1b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_NAME = market_name = \"intelligent virtual assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8d5e3b-2891-40ef-b25a-d4384733ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Intelligent Virtual Assistant Market 2025-2029_\n",
      "Global Intelligent Virtual Assistant Market 2025-2029_\n",
      "intelligent virtual assistant\n"
     ]
    }
   ],
   "source": [
    "print(initial_prefix)\n",
    "print(prefix)\n",
    "print(market_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ae267996-afac-463a-a4fc-28ed133aa08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read file: C:\\Users\\Madhan\\Write\\xlsx (40)\n",
      "Data written to: C:\\Users\\Madhan\\Write Output\\Write.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Read location (unchanged)\n",
    "input_folder = r'C:\\Users\\Madhan\\Write'\n",
    "\n",
    "# Output location and filename (updated)\n",
    "output_file = r'C:\\Users\\Madhan\\Write Output\\Write.xlsx'\n",
    "\n",
    "# Function to detect encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(2048)\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding'] or 'utf-8'\n",
    "\n",
    "# Try reading the file in different formats\n",
    "def try_read_file(file_path):\n",
    "    try:\n",
    "        return pd.read_excel(file_path, sheet_name=None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, encoding=encoding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, sep='\\t', encoding=encoding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_json(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_xml(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Main function to read from input folder and save Excel to output folder\n",
    "def convert_first_file_to_excel(input_folder, output_file):\n",
    "    files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files found in the input folder.\")\n",
    "        return\n",
    "\n",
    "    file_path = os.path.join(input_folder, files[0])\n",
    "    print(f\"Trying to read file: {file_path}\")\n",
    "\n",
    "    data = try_read_file(file_path)\n",
    "\n",
    "    if data is None:\n",
    "        print(f\"Could not find a readable file format in {file_path}\")\n",
    "        return\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        if isinstance(data, dict):  # Multiple Excel sheets\n",
    "            for sheet_name, df in data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "        else:\n",
    "            data.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    print(f\"Data written to: {output_file}\")\n",
    "\n",
    "# Run it\n",
    "convert_first_file_to_excel(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9125d907-3bbf-40ac-8c09-367180fe4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: C:\\Users\\Madhan\\Write Output\\Write Output1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Step 1: Directly read the specified Excel file\n",
    "input_file = r\"C:\\Users\\Madhan\\Write Output\\Write.xlsx\"\n",
    "\n",
    "# Attempt to read the file\n",
    "try:\n",
    "    df = pd.read_excel(input_file, header=None)\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"Unable to read the Excel file at {input_file}. Error: {e}\")\n",
    "\n",
    "# Step 2: Convert first two rows to columns\n",
    "row1 = df.iloc[0].tolist()\n",
    "row2 = df.iloc[1].tolist()\n",
    "\n",
    "# Step 3: Create DataFrame with \"Header\", \"Content\", \"Copy\"\n",
    "output_df = pd.DataFrame({\n",
    "    'Header': row1,\n",
    "    'Content': row2\n",
    "})\n",
    "output_df['Copy'] = output_df['Content']  # Column C is a copy of Column B\n",
    "\n",
    "# === Trim whitespace in Header column ===\n",
    "output_df['Header'] = output_df['Header'].astype(str).str.strip()\n",
    "\n",
    "# Step 4: Save to Write Output1.xlsx in the same directory\n",
    "output_path = os.path.join(os.path.dirname(input_file), \"Write Output1.xlsx\")\n",
    "\n",
    "# Use openpyxl to retain control over Excel layout\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Sheet1\"\n",
    "\n",
    "# Insert headers\n",
    "ws.append([\"Header\", \"Content\", \"Copy\"])\n",
    "\n",
    "# Insert data\n",
    "for index, row in output_df.iterrows():\n",
    "    ws.append([row['Header'], row['Content'], row['Copy']])\n",
    "\n",
    "wb.save(output_path)\n",
    "print(f\"File saved successfully at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b6b8b486-c925-467c-9e55-bd8187b2928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Cleaned and paragraph-preserved content saved to: C:\\Users\\Madhan\\Write Output\\Write Output2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Clean HTML while preserving paragraph and line breaks\n",
    "def clean_html_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Decode HTML entities like &bull;\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Replace <br> tags with single line breaks\n",
    "    text = re.sub(r'<br\\s*/?>', '\\n', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace <p> tags with nothing; close </p> with double line break to separate paragraphs\n",
    "    text = re.sub(r'<p\\s*>', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'</p\\s*>', '\\n\\n', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace <li> bullets with • and line breaks\n",
    "    text = text.replace(\"&bull;\", \"•\").replace(\"&#8226;\", \"•\")\n",
    "    text = re.sub(r'<li\\s*>', '• ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'</li\\s*>', '\\n', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove any other remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Normalize: no triple line breaks, clean spaces\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # max two line breaks for paragraph\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    cleaned = '\\n'.join(lines).strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r\"C:\\Users\\Madhan\\Write Output\\Write Output1.xlsx\"\n",
    "wb = load_workbook(file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Process Column C (starting from row 2)\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    original_text = ws.cell(row=row, column=3).value\n",
    "    if original_text:\n",
    "        cleaned_text = clean_html_content(original_text)\n",
    "        ws.cell(row=row, column=3, value=cleaned_text)\n",
    "\n",
    "# Save the result\n",
    "output_path = r\"C:\\Users\\Madhan\\Write Output\\Write Output2.xlsx\"\n",
    "wb.save(output_path)\n",
    "print(f\"✔ Cleaned and paragraph-preserved content saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "804b1166-4a2e-411d-82b5-3910275612fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Excel saved to: C:\\Users\\Madhan\\Write Output\\Write Final Excel1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "main_file_path = r'C:\\Users\\Madhan\\Write Output\\Write Final Excel.xlsx'\n",
    "source_file_path = r'C:\\Users\\Madhan\\Write Output\\Write Output2.xlsx'\n",
    "output_file_path = r'C:\\Users\\Madhan\\Write Output\\Write Final Excel1.xlsx'\n",
    "\n",
    "# Load both workbooks (assuming data is in the first sheet)\n",
    "main_df = pd.read_excel(main_file_path)\n",
    "source_df = pd.read_excel(source_file_path)\n",
    "\n",
    "# Rename the columns for clarity (optional but recommended)\n",
    "main_df.columns = [col.strip() for col in main_df.columns]\n",
    "source_df.columns = [col.strip() for col in source_df.columns]\n",
    "\n",
    "# Merge based on the unique ID in Column A (assumed to be the first column)\n",
    "# Assuming Column A is named the same in both files\n",
    "key_column = main_df.columns[0]  # Get the name of the first column\n",
    "column_to_merge = source_df.columns[2]  # Third column (Column C)\n",
    "\n",
    "# Merge the Column C from source into the main DataFrame\n",
    "merged_df = pd.merge(main_df, source_df[[key_column, column_to_merge]], on=key_column, how='left')\n",
    "\n",
    "# Save the merged data to a new Excel file\n",
    "merged_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'Merged Excel saved to: {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1a6568ad-fc39-43e4-9c29-409f2483db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhan\\AppData\\Local\\Temp\\ipykernel_8580\\3439352617.py:44: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  header = str(row[0]).strip()\n",
      "C:\\Users\\Madhan\\AppData\\Local\\Temp\\ipykernel_8580\\3439352617.py:45: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  content = str(row[1]).strip() if not pd.isna(row[1]) else \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document saved successfully at: C:\\Users\\Madhan\\Write Output\\Write.docx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.style import WD_STYLE_TYPE\n",
    "from docx.shared import Pt\n",
    "\n",
    "# Load Excel file\n",
    "excel_file = r'C:\\Users\\Madhan\\Write Output\\Write Final Excel1.xlsx'  # Replace with your file\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Create a new Word Document\n",
    "doc = Document()\n",
    "\n",
    "# Ensure styles exist\n",
    "styles = doc.styles\n",
    "\n",
    "# Ensure Heading 1 exists and modify if needed\n",
    "heading1_style = styles['Heading 1']\n",
    "heading1_font = heading1_style.font\n",
    "heading1_font.name = 'Arial'\n",
    "heading1_font.size = Pt(14)\n",
    "\n",
    "# Heading 2 style\n",
    "heading2_style = styles['Heading 2']\n",
    "heading2_font = heading2_style.font\n",
    "heading2_font.name = 'Arial'\n",
    "heading2_font.size = Pt(12)\n",
    "\n",
    "# Normal style\n",
    "normal_style = styles['Normal']\n",
    "normal_font = normal_style.font\n",
    "normal_font.name = 'Arial'\n",
    "normal_font.size = Pt(11)\n",
    "\n",
    "# Specific headers to mark as Heading 1\n",
    "heading1_titles = [\n",
    "    \"Market Definition\", \"Year-over-year growth 2020 - 2025\", \"Five Force Analysis\",\n",
    "    \"Market Segmentation by\", \"Market Segmentation by Geography\", \"Market Drivers\",\n",
    "    \"Market Challenges\", \"Market Trends\", \"Vendor Landscape\", \"Objective\",\n",
    "    \"Notes and caveats\", \"AI Content\", \"SWOT\", \"Key Offering\", \"Appendix\"\n",
    "]\n",
    "\n",
    "# Iterate over Excel rows\n",
    "for index, row in df.iterrows():\n",
    "    header = str(row[0]).strip()\n",
    "    content = str(row[1]).strip() if not pd.isna(row[1]) else \"\"\n",
    "\n",
    "    # Apply Heading 1 or Heading 2 style\n",
    "    if header in heading1_titles:\n",
    "        doc.add_paragraph(header, style='Heading 1')\n",
    "    else:\n",
    "        doc.add_paragraph(header, style='Heading 2')\n",
    "    \n",
    "    # Add the content in Normal style\n",
    "    if content:\n",
    "        doc.add_paragraph(content, style='Normal')\n",
    "\n",
    "# Save the Word document at the desired location\n",
    "output_path = r'C:\\Users\\Madhan\\Write Output\\Write.docx'\n",
    "doc.save(output_path)\n",
    "\n",
    "print(f\"Document saved successfully at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3d0a90ba-edff-4932-8838-6cc25788a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read file: C:\\Users\\Madhan\\Vendor\\xlsx (39)\n",
      "Data written to: C:\\Users\\Madhan\\Vendor Output\\Vendor List.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Read location (unchanged)\n",
    "input_folder = r'C:\\Users\\Madhan\\Vendor'\n",
    "\n",
    "# Output location and filename (updated)\n",
    "output_file = r'C:\\Users\\Madhan\\Vendor Output\\Vendor List.xlsx'\n",
    "\n",
    "# Function to detect encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(2048)\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result['encoding'] or 'utf-8'\n",
    "\n",
    "# Try reading the file in different formats\n",
    "def try_read_file(file_path):\n",
    "    try:\n",
    "        return pd.read_excel(file_path, sheet_name=None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, encoding=encoding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, sep='\\t', encoding=encoding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_json(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_xml(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Main function to read from input folder and save Excel to output folder\n",
    "def convert_first_file_to_excel(input_folder, output_file):\n",
    "    files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files found in the input folder.\")\n",
    "        return\n",
    "\n",
    "    file_path = os.path.join(input_folder, files[0])\n",
    "    print(f\"Trying to read file: {file_path}\")\n",
    "\n",
    "    data = try_read_file(file_path)\n",
    "\n",
    "    if data is None:\n",
    "        print(f\"Could not find a readable file format in {file_path}\")\n",
    "        return\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        if isinstance(data, dict):  # Multiple Excel sheets\n",
    "            for sheet_name, df in data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "        else:\n",
    "            data.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    print(f\"Data written to: {output_file}\")\n",
    "\n",
    "# Run it\n",
    "convert_first_file_to_excel(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97905967-a95c-4065-bdb7-2dacf61f94a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel file: C:\\Users\\Madhan\\Vendor Output\\Vendor List.xlsx\n",
      "\n",
      "Partial Matches (Unmatched IDs with Vendors Found):\n",
      "No partial matches found.\n",
      "\n",
      "Unmatched IDs (No match found):\n",
      "+----------------+\n",
      "| Unmatched ID   |\n",
      "+================+\n",
      "| Summary        |\n",
      "+----------------+\n",
      "\n",
      "Data transfer and merge completed successfully! Updated file saved at: C:\\Users\\Madhan\\Vendor Output\\Vendor Merged.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from tabulate import tabulate\n",
    "from colorama import init, Fore, Back, Style\n",
    "\n",
    "# Initialize colorama for console colors\n",
    "init()\n",
    "\n",
    "# Input directory and file\n",
    "input_dir = r\"C:\\Users\\Madhan\\Vendor Output\"\n",
    "input_file = None\n",
    "\n",
    "# Find first Excel file in directory\n",
    "for file in os.listdir(input_dir):\n",
    "    full_path = os.path.join(input_dir, file)\n",
    "    if os.path.isfile(full_path) and (file.lower().endswith(('.xlsx', '.xls')) or '.' not in file):\n",
    "        try:\n",
    "            pd.read_excel(full_path, header=None)\n",
    "            input_file = full_path\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "if not input_file:\n",
    "    raise FileNotFoundError(\"No readable Excel workbook found in the folder.\")\n",
    "\n",
    "print(f\"Reading Excel file: {input_file}\")\n",
    "\n",
    "# Load workbook\n",
    "wb = load_workbook(input_file)\n",
    "\n",
    "# Create Summary sheet\n",
    "if 'Summary' in wb.sheetnames:\n",
    "    del wb['Summary']\n",
    "summary_sheet = wb.create_sheet('Summary')\n",
    "summary_sheet['A1'] = 'Sheet Name'\n",
    "summary_sheet['B1'] = 'Row 2 Data'\n",
    "\n",
    "# Populate Summary sheet from all sheets except first one\n",
    "summary_row = 2\n",
    "for sheet in wb.sheetnames[1:]:\n",
    "    ws = wb[sheet]\n",
    "    row_data = [cell.value for cell in ws[2]]\n",
    "    sheet_name_cleaned = sheet.split(' ', 1)[0]\n",
    "    summary_sheet[f'A{summary_row}'] = sheet_name_cleaned\n",
    "    for col_idx, value in enumerate(row_data, start=2):\n",
    "        summary_sheet.cell(row=summary_row, column=col_idx, value=value)\n",
    "    summary_row += 1\n",
    "\n",
    "# Normalize helper\n",
    "def normalize_key(key):\n",
    "    if isinstance(key, str):\n",
    "        return key.strip().lower()\n",
    "    return key\n",
    "\n",
    "# Build dict from first sheet keyed by normalized ID in col A\n",
    "first_sheet = wb[wb.sheetnames[0]]\n",
    "first_sheet_dict = {}\n",
    "for row in first_sheet.iter_rows(min_row=2, max_row=first_sheet.max_row):\n",
    "    unique_id = normalize_key(row[0].value)\n",
    "    first_sheet_dict[unique_id] = [cell.value for cell in row]\n",
    "\n",
    "# Read Vendors Covered sheet column A for vendor list (normalized)\n",
    "if 'Vendors Covered' in wb.sheetnames:\n",
    "    vendors_sheet = wb['Vendors Covered']\n",
    "    vendors_list = [str(cell.value).lower() for cell in vendors_sheet['A'] if cell.value]\n",
    "else:\n",
    "    vendors_list = []\n",
    "\n",
    "# Function to find partial matches in vendors list\n",
    "def find_partial_match(key, vendor_names):\n",
    "    key_lower = key.lower() if key else ''\n",
    "    return [v for v in vendor_names if key_lower and key_lower in v]\n",
    "\n",
    "partial_match_log = []  # tuples of (unmatched_id, matched_vendors)\n",
    "unmatched_ids = []\n",
    "\n",
    "# Merge data into Summary sheet from first_sheet_dict with partial matching fallback\n",
    "for summary_row in summary_sheet.iter_rows(min_row=2, max_row=summary_sheet.max_row):\n",
    "    summary_unique_id_raw = summary_row[0].value\n",
    "    summary_unique_id = normalize_key(summary_unique_id_raw)\n",
    "\n",
    "    if summary_unique_id in first_sheet_dict:\n",
    "        first_sheet_row_data = first_sheet_dict[summary_unique_id]\n",
    "    else:\n",
    "        # Try partial match against vendor list\n",
    "        partial_matches = find_partial_match(summary_unique_id, vendors_list)\n",
    "        if partial_matches:\n",
    "            partial_match_log.append((summary_unique_id_raw, partial_matches))\n",
    "            # Try to find matching key in first_sheet_dict containing any partial vendor string\n",
    "            matched_key = None\n",
    "            for pm in partial_matches:\n",
    "                candidates = [k for k in first_sheet_dict.keys() if pm in k]\n",
    "                if candidates:\n",
    "                    matched_key = candidates[0]\n",
    "                    break\n",
    "            if matched_key:\n",
    "                first_sheet_row_data = first_sheet_dict[matched_key]\n",
    "            else:\n",
    "                first_sheet_row_data = None\n",
    "                unmatched_ids.append(summary_unique_id_raw)\n",
    "        else:\n",
    "            first_sheet_row_data = None\n",
    "            unmatched_ids.append(summary_unique_id_raw)\n",
    "\n",
    "    # Write merged data if found\n",
    "    if first_sheet_row_data:\n",
    "        start_col = len(summary_row) + 1\n",
    "        for col_idx, value in enumerate(first_sheet_row_data, start=start_col):\n",
    "            summary_sheet.cell(row=summary_row[0].row, column=col_idx, value=value)\n",
    "\n",
    "# Print partial matches and unmatched IDs with red background + white text in console\n",
    "print(Back.RED + Fore.WHITE + \"\\nPartial Matches (Unmatched IDs with Vendors Found):\" + Style.RESET_ALL)\n",
    "if partial_match_log:\n",
    "    table_pm = [(uid, \", \".join(vendors)) for uid, vendors in partial_match_log]\n",
    "    print(Back.RED + Fore.WHITE + tabulate(table_pm, headers=[\"Unmatched ID\", \"Matched Vendors\"], tablefmt=\"grid\") + Style.RESET_ALL)\n",
    "else:\n",
    "    print(Back.RED + Fore.WHITE + \"No partial matches found.\" + Style.RESET_ALL)\n",
    "\n",
    "print(Back.RED + Fore.WHITE + \"\\nUnmatched IDs (No match found):\" + Style.RESET_ALL)\n",
    "if unmatched_ids:\n",
    "    table_um = [(uid,) for uid in unmatched_ids]\n",
    "    print(Back.RED + Fore.WHITE + tabulate(table_um, headers=[\"Unmatched ID\"], tablefmt=\"grid\") + Style.RESET_ALL)\n",
    "else:\n",
    "    print(Back.RED + Fore.WHITE + \"No unmatched IDs.\" + Style.RESET_ALL)\n",
    "\n",
    "# --- Create Merged Output sheet from Summary ---\n",
    "\n",
    "if 'Merged Output' in wb.sheetnames:\n",
    "    del wb['Merged Output']\n",
    "\n",
    "merged_sheet = wb.create_sheet('Merged Output')\n",
    "\n",
    "# Set headers\n",
    "merged_sheet['A1'] = 'Domain'\n",
    "merged_sheet['B1'] = 'Company'\n",
    "merged_sheet['C1'] = 'Description'\n",
    "\n",
    "# Copy columns A, G, C from Summary to Merged Output with new headers\n",
    "for row_idx, row in enumerate(summary_sheet.iter_rows(min_row=2, max_row=summary_sheet.max_row), start=2):\n",
    "    domain = row[0].value      # Col A in Summary\n",
    "    company = row[6].value     # Col G in Summary (index 6 means col 7)\n",
    "    description = row[2].value # Col C in Summary\n",
    "\n",
    "    merged_sheet.cell(row=row_idx, column=1, value=domain)\n",
    "    merged_sheet.cell(row=row_idx, column=2, value=company)\n",
    "    merged_sheet.cell(row=row_idx, column=3, value=description)\n",
    "\n",
    "# Remove last row in Merged Output sheet (if > 1)\n",
    "if merged_sheet.max_row > 1:\n",
    "    merged_sheet.delete_rows(merged_sheet.max_row)\n",
    "\n",
    "# Extract data rows to sort by Company\n",
    "data_rows = list(merged_sheet.iter_rows(min_row=2, max_row=merged_sheet.max_row, max_col=3, values_only=True))\n",
    "\n",
    "# Sort by Company (col 2)\n",
    "data_rows_sorted = sorted(data_rows, key=lambda x: (x[1] is None, str(x[1]).lower() if x[1] else ''))\n",
    "\n",
    "# Clear old data rows\n",
    "merged_sheet.delete_rows(2, merged_sheet.max_row)\n",
    "\n",
    "# Write sorted data back\n",
    "for i, row_data in enumerate(data_rows_sorted, start=2):\n",
    "    merged_sheet.cell(row=i, column=1, value=row_data[0])\n",
    "    merged_sheet.cell(row=i, column=2, value=row_data[1])\n",
    "    merged_sheet.cell(row=i, column=3, value=row_data[2])\n",
    "\n",
    "# --- Color red background for unmatched and partial match domains in Merged Output ---\n",
    "\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "\n",
    "# Collect normalized domain values to color\n",
    "domains_to_color = set()\n",
    "domains_to_color.update(uid.strip().lower() for uid in unmatched_ids if uid)\n",
    "domains_to_color.update(uid.strip().lower() for uid, _ in partial_match_log if uid)\n",
    "\n",
    "for row in merged_sheet.iter_rows(min_row=2, max_row=merged_sheet.max_row, min_col=1, max_col=1):\n",
    "    cell = row[0]\n",
    "    if cell.value and cell.value.strip().lower() in domains_to_color:\n",
    "        cell.fill = red_fill\n",
    "\n",
    "# Move 'Merged Output' sheet to be the 2nd sheet (index 1)\n",
    "sheets = wb._sheets\n",
    "sheets.insert(1, sheets.pop(sheets.index(merged_sheet)))\n",
    "\n",
    "# Save output\n",
    "output_file = os.path.join(input_dir, 'Vendor Merged.xlsx')\n",
    "wb.save(output_file)\n",
    "\n",
    "print(f\"\\nData transfer and merge completed successfully! Updated file saved at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8b37d607-5bf2-45eb-98d9-776343d9e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File A updated and saved: C:\\Users\\Madhan\\Vendor Output\\Vendor Merged.xlsx\n",
      "✅ No unmatched domains found to append to File B.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# File paths\n",
    "file_a = r\"C:\\Users\\Madhan\\Vendor Output\\Vendor Merged.xlsx\"\n",
    "file_b = r\"C:\\Users\\Madhan\\Combined_Vendors_Covered.xlsx\"\n",
    "\n",
    "# Make sure both files exist\n",
    "if not os.path.exists(file_a):\n",
    "    print(f\"❌ Error: File A not found at {file_a}\")\n",
    "elif not os.path.exists(file_b):\n",
    "    print(f\"❌ Error: File B not found at {file_b}\")\n",
    "else:\n",
    "    # Load B file's \"Domain\" values from Sheet1\n",
    "    try:\n",
    "        df_b = pd.read_excel(file_b, sheet_name=\"Sheet1\", usecols=[\"Domain\"])\n",
    "        b_domains = set(df_b[\"Domain\"].dropna().astype(str).str.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading File B: {e}\")\n",
    "        b_domains = set()\n",
    "\n",
    "    new_domains = []  # To collect unmatched domains to append to B\n",
    "\n",
    "    # Load File A workbook\n",
    "    try:\n",
    "        wb_a = load_workbook(file_a)\n",
    "        if \"Vendors Covered\" not in wb_a.sheetnames:\n",
    "            print(f\"❌ 'Vendors Covered' sheet not found in File A.\")\n",
    "        else:\n",
    "            ws_a = wb_a[\"Vendors Covered\"]\n",
    "\n",
    "            # Define fill colors\n",
    "            green_fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\n",
    "            red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "\n",
    "            # Get column index of \"Domain\"\n",
    "            header = [cell.value for cell in ws_a[1]]\n",
    "            if \"Domain\" not in header:\n",
    "                print(\"❌ 'Domain' column not found in File A.\")\n",
    "            else:\n",
    "                domain_col_idx = header.index(\"Domain\") + 1  # openpyxl uses 1-based indexing\n",
    "\n",
    "                # Loop through rows\n",
    "                for row in range(2, ws_a.max_row + 1):\n",
    "                    cell = ws_a.cell(row=row, column=domain_col_idx)\n",
    "                    domain_value = str(cell.value).strip() if cell.value else \"\"\n",
    "                    if domain_value in b_domains:\n",
    "                        cell.fill = green_fill\n",
    "                    elif domain_value:  # Non-empty unmatched value\n",
    "                        cell.fill = red_fill\n",
    "                        new_domains.append(domain_value)\n",
    "\n",
    "                # Save A file\n",
    "                wb_a.save(file_a)\n",
    "                print(f\"✅ File A updated and saved: {file_a}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing File A: {e}\")\n",
    "\n",
    "    # Append unmatched domains to B\n",
    "    if new_domains:\n",
    "        try:\n",
    "            wb_b = load_workbook(file_b)\n",
    "            if \"Sheet1\" not in wb_b.sheetnames:\n",
    "                print(f\"❌ 'Sheet1' sheet not found in File B.\")\n",
    "            else:\n",
    "                ws_b = wb_b[\"Sheet1\"]\n",
    "\n",
    "                # Get column index for \"Domain\"\n",
    "                header_b = [cell.value for cell in ws_b[1]]\n",
    "                if \"Domain\" not in header_b:\n",
    "                    print(\"❌ 'Domain' column not found in File B (Sheet1).\")\n",
    "                else:\n",
    "                    domain_col_b = header_b.index(\"Domain\") + 1\n",
    "                    max_row_b = ws_b.max_row\n",
    "\n",
    "                    for domain in new_domains:\n",
    "                        max_row_b += 1\n",
    "                        ws_b.cell(row=max_row_b, column=domain_col_b, value=domain)\n",
    "\n",
    "                    wb_b.save(file_b)\n",
    "                    print(f\"✅ Unmatched domains appended and File B saved: {file_b}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error appending to File B: {e}\")\n",
    "    else:\n",
    "        print(\"✅ No unmatched domains found to append to File B.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7f6c4dbe-2d6a-4249-9229-050ee5d941eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files copied to 'C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_' with prefix 'Global Intelligent Virtual Assistant Market 2025-2029_'\n",
      "New prefix used: Global Intelligent Virtual Assistant Market 2025-2029_\n",
      "✅ Excel saved at: C:\\Users\\Madhan\\Write Output\\Write Files\\Global Intelligent Virtual Assistant Market 2025-2029_Write Output1.xlsx\n",
      "Reading: C:\\Users\\Madhan\\Write Output\\Write Files\\Global Intelligent Virtual Assistant Market 2025-2029_Write Output1.xlsx\n",
      "Written converted file: C:\\Users\\Madhan\\Write Output\\Write Files\\Global Intelligent Virtual Assistant Market 2025-2029_Write Output1_converted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Detect file encoding\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(2048)\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding'] or 'utf-8'\n",
    "\n",
    "# Try reading file in various formats\n",
    "def try_read_file(file_path):\n",
    "    try:\n",
    "        return pd.read_excel(file_path, sheet_name=None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        enc = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, encoding=enc)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        enc = detect_encoding(file_path)\n",
    "        return pd.read_csv(file_path, sep='\\t', encoding=enc)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_json(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_xml(file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Gather relevant files to copy\n",
    "def gather_files_to_copy(user_folder, word_folder, vendor_folder, swot_folder, prefix):\n",
    "    files = []\n",
    "    for folder in [user_folder, word_folder, vendor_folder, swot_folder]:\n",
    "        for f in os.listdir(folder):\n",
    "            full = os.path.join(folder, f)\n",
    "            if os.path.isfile(full):\n",
    "                if folder == word_folder and not f.lower().endswith('.docx'):\n",
    "                    continue\n",
    "                if folder == vendor_folder and not (f.startswith(\"Vendor Merged\") and f.lower().endswith(('.xlsx', '.xls'))):\n",
    "                    continue\n",
    "                if folder == swot_folder and not f.startswith(\"SWOT Load Template\"):\n",
    "                    continue\n",
    "                files.append(full)\n",
    "    # Add files starting with prefix from user_folder\n",
    "    for f in os.listdir(user_folder):\n",
    "        if f.startswith(prefix):\n",
    "            files.append(os.path.join(user_folder, f))\n",
    "    return files\n",
    "\n",
    "def prefix_exists(path, prefix):\n",
    "    return any(f.startswith(prefix) for f in os.listdir(path))\n",
    "\n",
    "# Copy files, eliminate old prefixed, return final prefix\n",
    "def copy_and_rename(files, dest_root, prefix):\n",
    "    write_path = r\"C:\\Users\\Madhan\\Write Output\\Write Files\"\n",
    "    download_path = dest_root\n",
    "\n",
    "    while (os.path.exists(os.path.join(download_path, prefix)) or\n",
    "           prefix_exists(write_path, prefix) or\n",
    "           prefix_exists(download_path, prefix)):\n",
    "        print(f\"⚠️ Prefix '{prefix}' already exists; please specify a new one:\")\n",
    "        prefix = input(\"Enter new prefix (include trailing underscore if desired): \")\n",
    "\n",
    "    target = os.path.join(dest_root, prefix)\n",
    "    os.makedirs(target, exist_ok=True)\n",
    "\n",
    "    for existing in os.listdir(target):\n",
    "        if existing.startswith(prefix):\n",
    "            os.remove(os.path.join(target, existing))\n",
    "\n",
    "    for src in files:\n",
    "        fname = os.path.basename(src)\n",
    "        new = fname if fname.startswith(prefix) else f\"{prefix}{fname}\"\n",
    "        shutil.copy2(src, os.path.join(target, new))\n",
    "\n",
    "    print(f\"✅ Files copied to '{target}' with prefix '{prefix}'\")\n",
    "    return prefix\n",
    "\n",
    "# Process and convert functions\n",
    "def process_excel_file(inp, outp):\n",
    "    df = pd.read_excel(inp)\n",
    "    df.to_excel(outp, index=False, engine='openpyxl')\n",
    "    print(f\"✅ Excel saved at: {outp}\")\n",
    "\n",
    "def convert_to_excel(inp, outp):\n",
    "    if not inp.lower().endswith('.xlsx'):\n",
    "        df = pd.read_csv(inp)\n",
    "        df.to_excel(outp, index=False, engine='openpyxl')\n",
    "        print(f\"✅ Converted to Excel: {outp}\")\n",
    "\n",
    "def convert_first_file_to_excel(folder, prefix):\n",
    "    candidates = [f for f in os.listdir(folder) if f.startswith(prefix)]\n",
    "    if not candidates:\n",
    "        print(f\"No files found starting with prefix '{prefix}'\")\n",
    "        return\n",
    "    target = candidates[0]\n",
    "    fp = os.path.join(folder, target)\n",
    "    print(f\"Reading: {fp}\")\n",
    "    data = try_read_file(fp)\n",
    "    if data is None:\n",
    "        print(f\"No readable format in {fp}\")\n",
    "        return\n",
    "\n",
    "    outp = os.path.join(folder, target.split('.')[0] + '_converted.xlsx')\n",
    "    with pd.ExcelWriter(outp, engine='openpyxl') as writer:\n",
    "        if isinstance(data, dict):\n",
    "            for sheet_name, df in data.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "        else:\n",
    "            data.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    print(f\"Written converted file: {outp}\")\n",
    "\n",
    "# Main workflow returns final prefix\n",
    "def main():\n",
    "    user_src = r\"C:\\Users\\Madhan\\Write\"\n",
    "    word_folder = r\"C:\\Users\\Madhan\\Write Output\"\n",
    "    vendor_folder = r\"C:\\Users\\Madhan\\Vendor Output\"\n",
    "    swot_folder = r\"C:\\Users\\Madhan\"\n",
    "    downloads = r\"C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\"\n",
    "    write_files = r\"C:\\Users\\Madhan\\Write Output\\Write Files\"\n",
    "\n",
    "    files = gather_files_to_copy(user_src, word_folder, vendor_folder, swot_folder, initial_prefix)\n",
    "    final_prefix = copy_and_rename(files, downloads, initial_prefix)\n",
    "    print(f\"New prefix used: {final_prefix}\")\n",
    "\n",
    "    inp_excel = r\"C:\\Users\\Madhan\\Write Output\\Write Output1.xlsx\"\n",
    "    out_excel = os.path.join(write_files, f\"{final_prefix}Write Output1.xlsx\")\n",
    "    process_excel_file(inp_excel, out_excel)\n",
    "\n",
    "    for f in os.listdir(write_files):\n",
    "        src = os.path.join(write_files, f)\n",
    "        if os.path.isfile(src):\n",
    "            tgt = os.path.join(write_files, f\"{final_prefix}{os.path.splitext(f)[0]}.xlsx\")\n",
    "            convert_to_excel(src, tgt)\n",
    "\n",
    "    convert_first_file_to_excel(write_files, final_prefix)\n",
    "    return final_prefix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prefix = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2832c8-d930-43bf-ac48-304a725b218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3faee92a-b264-41e0-a3f2-d2c7a3fe9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final prefix used is: Global Intelligent Virtual Assistant Market 2025-2029_\n"
     ]
    }
   ],
   "source": [
    "print(f\"The final prefix used is: {prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "82ffdd00-3d1f-4ad6-a82c-ef63806eb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "###AI Instance content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7d23fc5e-fb51-4895-9792-209bb6259499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading original document: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\\Global Intelligent Virtual Assistant Market 2025-2029_Write.docx\n",
      "--> Already given market name: 'intelligent virtual assistant'\n",
      "\n",
      "Processing document paragraphs...\n",
      "\n",
      "--> Found start trigger 'market segmentation by'. Beginning analysis.\n",
      "--> Analyzing paragraph: 'Chatbots play a key role in various businesses, including re...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'Text-to-speech is a type of computer-generated simulation of...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'North America has a higher inclination toward the use of hig...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'The increasing demand for enhanced customer services in busi...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'Intelligent virtual assistants are developed with a focus on...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'The global healthcare sector is undergoing a digital revolut...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "--> Analyzing paragraph: 'The global intelligent virtual assistant market is fragmente...'\n",
      "--> Successfully appended supporting instance and conclusion.\n",
      "\n",
      "--> Found stop trigger 'market definition'. Halting analysis.\n",
      "\n",
      "================================================================================\n",
      "                        Analysis Complete\n",
      "================================================================================\n",
      "\n",
      "Success! A new file has been created with 7 supporting instance(s) added.\n",
      "Please open: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\\Global Intelligent Virtual Assistant Market 2025-2029_Write_AI Instances.docx\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script performs a structural analysis of a Word document. It activates\n",
    "after a specific trigger, appends a highly factual instance from a primary news\n",
    "or company source, adds a concluding sentence, and stops at an end phrase.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from docx import Document\n",
    "import time\n",
    "\n",
    "# --- Required Variables ---\n",
    "# Uses `prefix` from earlier cell\n",
    "# Example: prefix = \"ABC\"\n",
    "try:\n",
    "    prefix\n",
    "except NameError:\n",
    "    raise Exception(\"You must define 'prefix' in a cell above before running this code.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DIR = r\"C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\"\n",
    "ANALYSIS_START_TRIGGER = \"market segmentation by\"\n",
    "ANALYSIS_STOP_TRIGGER = \"market definition\"\n",
    "MIN_PARAGRAPH_WORD_COUNT = 20\n",
    "\n",
    "# --- File Paths ---\n",
    "FOLDER_PATH = os.path.join(BASE_DIR, prefix)\n",
    "INPUT_DOC_PATH = os.path.join(FOLDER_PATH, f\"{prefix}Write.docx\")\n",
    "OUTPUT_DOC_PATH = os.path.join(FOLDER_PATH, f\"{prefix}Write_AI Instances.docx\")\n",
    "\n",
    "def configure_ai_model():\n",
    "    \"\"\"Configures the Google AI model with the API key.\"\"\"\n",
    "    if API_KEY == \"YOUR_API_KEY_HERE\" or not API_KEY:\n",
    "        print(\"FATAL ERROR: The API_KEY is not set in the script.\")\n",
    "        return False\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not configure Google AI. Details: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_market_name(doc):\n",
    "    \"\"\"\n",
    "    Uses already given MARKET_NAME if available, otherwise tries to auto-detect it.\n",
    "    If both fail, asks the user to enter it.\n",
    "    \"\"\"\n",
    "    # 1. Use already provided MARKET_NAME if available and non-empty\n",
    "    try:\n",
    "        if \"MARKET_NAME\" in globals() and MARKET_NAME.strip():\n",
    "            print(f\"--> Already given market name: '{MARKET_NAME.strip()}'\")\n",
    "            return MARKET_NAME.strip()\n",
    "    except NameError:\n",
    "        pass  # MARKET_NAME not defined, move on to auto-detection\n",
    "\n",
    "    # 2. Try automatic detection from document\n",
    "    pattern = re.compile(r\"the (?:global\\s|)?([\\w\\s-]+?) market\", re.IGNORECASE)\n",
    "    for para in doc.paragraphs[:15]:\n",
    "        match = pattern.search(para.text)\n",
    "        if match:\n",
    "            market_name = match.group(1).strip()\n",
    "            print(f\"--> Automatically identified market name: '{market_name}'\")\n",
    "            return market_name\n",
    "\n",
    "    # 3. Fallback — ask the user\n",
    "    print(\"--> Warning: Could not automatically identify market name.\")\n",
    "    market_name = input(\"Please provide the market name: \").strip()\n",
    "    return market_name if market_name else \"global\"\n",
    "\n",
    "def is_paragraph_a_target(para_text):\n",
    "    \"\"\"\n",
    "    Determines if a paragraph is a suitable candidate for an AI-generated instance.\n",
    "    \"\"\"\n",
    "    text_lower = para_text.lower()\n",
    "    if len(text_lower.split()) < MIN_PARAGRAPH_WORD_COUNT:\n",
    "        return False\n",
    "    if text_lower.strip().startswith(\"for instance\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_ai_instance(paragraph_content, market_name):\n",
    "    \"\"\"\n",
    "    Generates a single, factually accurate instance from a primary news or company source.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "    prompt = f\"\"\"\n",
    "    You are a meticulous Factual Research Analyst. Your primary mission is to find a single, verifiable, real-world event to support an analytical statement.\n",
    "\n",
    "    **Market Context:** The {market_name} market\n",
    "    **Analytical Statement to Support:** \"{paragraph_content}\"\n",
    "\n",
    "    **Your Task and Source Requirements:**\n",
    "    Find exactly **one** specific event (e.g., a product launch, partnership, funding announcement, regulatory approval) that directly supports the analytical statement above.\n",
    "\n",
    "    - **PERMITTED SOURCES:** You must draw your information from highly credible, event-based sources such as official **company press releases**, major **third-party news platforms** (like Reuters, Bloomberg, TechCrunch, CNBC), or **regulatory filings** (like SEC reports).\n",
    "    - **FORBIDDEN SOURCES:** You are strictly prohibited from using information from other **market research reports**, analyst opinion pieces, blogs, or speculative articles.\n",
    "    - **FACTUAL ACCURACY:** All data points in your response, especially the **Month, Year, and any numbers or figures,** must be factually correct and verifiable from the source. Do not estimate or invent data.\n",
    "\n",
    "    **CRITICAL FORMATTING RULE:**\n",
    "    Your entire response MUST begin with the exact phrase \"For instance, in Month Year, \".\n",
    "\n",
    "    **Other Rules (Strictly Enforced):**\n",
    "    1.  **PRIMARY DIRECTIVE:** Your response MUST NOT contain any information related to the COVID-19 pandemic or the war in Ukraine.\n",
    "    2.  Provide only data that has proper sources and links—such that if I search the content on Google, it appears in the results and the event must have occurred in 2023 or 2024 or 2025.\n",
    "    3.  The response must be a single, concise paragraph.\n",
    "    4.  Your language must be professional, following the Chicago Manual of Style, with no contractions.\n",
    "    \n",
    "**INSTRUCTIONS:**\n",
    "1. Use data only from **2024, 2025**, or if unavailable, from **January 2023 onward**.\n",
    "2. Source must be verifiable: official press releases, Bloomberg, CNBC, Reuters, TechCrunch, etc.\n",
    "3. Do not use analyst blogs, unverified reports, or estimates.\n",
    "4. Start the response with **\"For instance, in [Month] [Year], ...\"**\n",
    "5. Exclude references to COVID-19 or the Ukraine war.\n",
    "6. Limit to one paragraph, formal tone.\n",
    "7. Do not use repeated instances.\n",
    "8. Give the sources and links below the instances.\n",
    "9. Instance examples (1).\"For instance, in July 2024, ZF Lifetec introduced advancements in passive safety technology to enhance occupant protection. The company redesigned the steering wheel airbag to deploy from the upper rim, improving effectiveness in complex steering wheel layouts.\n",
    "\"(2).\". For instance, in October 2023, Hunter Engineering Company and Totalkare, a major distributor of Hunter state-of-the-art heavy-duty wheel alignment equipment, announced a collaboration that would propel three products to the forefront of the commercial vehicle maintenance market.\n",
    " \"(3).\"For instance, S and P Global Mobility forecasts global light vehicle sales to reach 88.3 million units in 2024, a 2.7% increase from 2023. The United States is expected to see sales rise by 2.0% to 15.9 million units despite challenges like high interest rates and tight credit.\n",
    "\"(4).\"For instance, the Global EV Outlook 2024, published by the International Energy Agency (IEA), highlights strong momentum in the electric vehicle (EV) sector, with global EV sales surpassing 17 million units, a 25% year-on-year increase.\" Like these kind of instances only\n",
    "Return only the final instance paragraph. If no verifiable example exists, return nothing.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"--> Analyzing paragraph: '{paragraph_content[:60]}...'\")\n",
    "        response = model.generate_content(prompt)\n",
    "        time.sleep(2)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"--> An error occurred while communicating with the AI: {e}\")\n",
    "        time.sleep(2)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to read, analyze, and write the document.\"\"\"\n",
    "    if not configure_ai_model():\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(INPUT_DOC_PATH):\n",
    "        print(f\"FATAL ERROR: The input document was not found at: {INPUT_DOC_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Reading original document: {INPUT_DOC_PATH}\")\n",
    "    source_doc = Document(INPUT_DOC_PATH)\n",
    "    new_doc = Document()\n",
    "\n",
    "    market_name = extract_market_name(source_doc)\n",
    "    concluding_sentence = f\"Thus, such factors are expected to positively impact on the growth of this segment in the global {market_name} market during the forecast period.\"\n",
    "\n",
    "    processing_active = False\n",
    "    instances_added = 0\n",
    "\n",
    "    print(\"\\nProcessing document paragraphs...\")\n",
    "    for para in source_doc.paragraphs:\n",
    "        new_doc.add_paragraph(para.text, style=para.style)\n",
    "        para_text_lower = para.text.lower()\n",
    "\n",
    "        if processing_active and ANALYSIS_STOP_TRIGGER in para_text_lower:\n",
    "            processing_active = False\n",
    "            print(f\"\\n--> Found stop trigger '{ANALYSIS_STOP_TRIGGER}'. Halting analysis.\")\n",
    "            continue\n",
    "\n",
    "        if not processing_active and ANALYSIS_START_TRIGGER in para_text_lower:\n",
    "            processing_active = True\n",
    "            print(f\"\\n--> Found start trigger '{ANALYSIS_START_TRIGGER}'. Beginning analysis.\")\n",
    "            continue\n",
    "\n",
    "        if processing_active and is_paragraph_a_target(para.text):\n",
    "            ai_instance = generate_ai_instance(para.text, market_name)\n",
    "\n",
    "            if ai_instance and ai_instance.lower().startswith(\"for instance\"):\n",
    "                new_doc.add_paragraph(ai_instance)\n",
    "                new_doc.add_paragraph(concluding_sentence)\n",
    "                instances_added += 1\n",
    "                print(f\"--> Successfully appended supporting instance and conclusion.\")\n",
    "            else:\n",
    "                print(f\"--> AI response for this paragraph did not meet format/source requirements. Skipping.\")\n",
    "\n",
    "    try:\n",
    "        new_doc.save(OUTPUT_DOC_PATH)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"                        Analysis Complete\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nSuccess! A new file has been created with {instances_added} supporting instance(s) added.\")\n",
    "        print(f\"Please open: {OUTPUT_DOC_PATH}\")\n",
    "        print(\"=\"*80)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFATAL ERROR: Could not save the new document. Details: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452323a-496f-43c4-8d3e-40620b446792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d50a8-eab3-4279-aa84-2199320e0add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0cd20475-a4a0-4616-a12e-d2b88fee05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SWOT#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a34891fe-6506-4ce3-a7e9-ffbb32ec41cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Using input folder: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\n",
      "\n",
      "📥 Reading file: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\\Global Intelligent Virtual Assistant Market 2025-2029_Vendor Merged.xlsx\n",
      "📊 Found 20 companies\n",
      "🔍 Generating SWOT for: Ada Health GmbH\n",
      "🔍 Generating SWOT for: Alphabet Inc.\n",
      "🔍 Generating SWOT for: Amazon.com Inc.\n",
      "🔍 Generating SWOT for: Anboto Europe SL Co.\n",
      "⚠️ Attempt 1 failed for Anboto Europe SL Co.: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1. (waiting 5s)\n",
      "🔍 Generating SWOT for: Apple Inc.\n",
      "🔍 Generating SWOT for: Baidu Inc.\n",
      "🔍 Generating SWOT for: Creative Virtual Ltd.\n",
      "🔍 Generating SWOT for: eGain Corp.\n",
      "🔍 Generating SWOT for: Inbenta Holdings Inc.\n",
      "🔍 Generating SWOT for: Infermedica Sp. z o.o.\n",
      "🔍 Generating SWOT for: Interactions LLC\n",
      "🔍 Generating SWOT for: International Business Machines Corp.\n",
      "🔍 Generating SWOT for: Microsoft Corp.\n",
      "🔍 Generating SWOT for: Nuance Communications Inc.\n",
      "🔍 Generating SWOT for: Oracle Corp.\n",
      "🔍 Generating SWOT for: Orbita Inc.\n",
      "🔍 Generating SWOT for: Samsung Electronics Co. Ltd.\n",
      "🔍 Generating SWOT for: Sense.ly Corp.\n",
      "🔍 Generating SWOT for: True Image Interactive Inc.\n",
      "🔍 Generating SWOT for: Verint Systems Inc.\n",
      "\n",
      "✅ SWOT analysis saved to: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\\Global Intelligent Virtual Assistant Market 2025-2029_ SWOT.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "#API_KEY = \"AIzaSyBneAP9zHGsalKlex7cohxVTs9LLAgdRt8\"  # 🔐 Replace with your actual API key\n",
    "\n",
    "# Define prefix and build the full input folder path\n",
    "\n",
    "INPUT_FOLDER = os.path.join(r\"C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\", prefix\n",
    ")\n",
    "\n",
    "# Print and check if folder exists\n",
    "print(\"📂 Using input folder:\", INPUT_FOLDER)\n",
    "if not os.path.exists(INPUT_FOLDER):\n",
    "    raise FileNotFoundError(f\"❌ Input folder not found: {INPUT_FOLDER}\")\n",
    "\n",
    "INPUT_SHEET = \"Vendors Covered\"\n",
    "COMPANY_COL = \"Company Name\"\n",
    "OUTPUT_FILE = os.path.join(\n",
    "    r\"C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\", \n",
    "    prefix, \n",
    "    f\"{prefix} SWOT.xlsx\"\n",
    ")\n",
    "ERROR_LOG = r\"C:\\Users\\Madhan\\SWOT\\error_log.txt\"\n",
    "\n",
    "# --- AI Setup ---\n",
    "def configure_ai_model():\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        return genai.GenerativeModel('gemini-2.5-pro')\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not configure AI. {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Clean text ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[\\'\\\"“”‘’™©®]\", \"\", text)  # remove quotes and trademarks\n",
    "    text = re.sub(r\"–|—\", \"-\", text)           # replace em dash with hyphen\n",
    "    text = re.sub(r\"\\b(vendor|bold|italic|underline)\\b\", \"\", text, flags=re.I)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)           # normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# --- Expand abbreviations ---\n",
    "def expand_abbreviations(text):\n",
    "    expansions = {\n",
    "        \"AI\": \"artificial intelligence\",\n",
    "        \"R&D\": \"research and development\",\n",
    "        \"HR\": \"human resources\",\n",
    "        \"IT\": \"information technology\",\n",
    "        \"CEO\": \"chief executive officer\",\n",
    "        # Add more if needed\n",
    "    }\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(expansions.keys()) + r')\\b', flags=re.I)\n",
    "    def repl(m):\n",
    "        return expansions[m.group(0).upper()]\n",
    "    return pattern.sub(repl, text)\n",
    "\n",
    "# --- Generate SWOT for a single company with retries and fallback ---\n",
    "def generate_swot(model, original_company_name, max_retries=10, base_delay=5):\n",
    "    prompt = f\"\"\"\n",
    "Prepare SWOT analysis for the company {original_company_name} in a table format with two sets of SWOT points.\n",
    "Each set should be one row with exactly one Strength, one Weakness, one Opportunity, and one Threat.\n",
    "Do not use numbering or bullet points.\n",
    "Expand all abbreviations.\n",
    "Remove all apostrophes, quotes, em dashes, trademark symbols, and formatting words like vendor, bold, italic, underline.\n",
    "Return output as plain text in tab-separated columns with headers:\n",
    "Company Name\\tStrength\\tWeakness\\tOpportunity\\tThreat\n",
    "The output must have exactly two rows for {original_company_name}, one for each SWOT set.\n",
    "\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_text = response.text.strip() if hasattr(response, \"text\") else str(response).strip()\n",
    "\n",
    "            lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "            swot_rows = []\n",
    "\n",
    "            if lines and lines[0].lower().startswith(\"company name\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for line in lines:\n",
    "                cols = line.split('\\t')\n",
    "                if len(cols) == 5:\n",
    "                    strength = expand_abbreviations(clean_text(cols[1]))\n",
    "                    weakness = expand_abbreviations(clean_text(cols[2]))\n",
    "                    opportunity = expand_abbreviations(clean_text(cols[3]))\n",
    "                    threat = expand_abbreviations(clean_text(cols[4]))\n",
    "                    swot_rows.append({\n",
    "                        \"Company Name\": original_company_name,\n",
    "                        \"Strength\": strength,\n",
    "                        \"Weakness\": weakness,\n",
    "                        \"Opportunity\": opportunity,\n",
    "                        \"Threat\": threat,\n",
    "                    })\n",
    "\n",
    "            if len(swot_rows) == 2:\n",
    "                return swot_rows\n",
    "            else:\n",
    "                raise ValueError(\"SWOT format invalid or incomplete\")\n",
    "\n",
    "        except Exception as e:\n",
    "            wait_time = base_delay * attempt\n",
    "            print(f\"⚠️ Attempt {attempt} failed for {original_company_name}: {e} (waiting {wait_time}s)\")\n",
    "            with open(ERROR_LOG, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"\\n---\\nCompany: {original_company_name}\\nAttempt: {attempt}\\nError: {e}\\nPrompt:\\n{prompt}\\n\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    # Fallback: ensure company still has SWOT\n",
    "    print(f\"🛑 All attempts failed for {original_company_name}, using fallback SWOT.\")\n",
    "    fallback_rows = [\n",
    "        {\n",
    "            \"Company Name\": original_company_name,\n",
    "            \"Strength\": f\"{original_company_name} has a strong market reputation and customer trust.\",\n",
    "            \"Weakness\": f\"{original_company_name} may face operational inefficiencies.\",\n",
    "            \"Opportunity\": f\"{original_company_name} could expand into emerging markets and digital platforms.\",\n",
    "            \"Threat\": f\"{original_company_name} is exposed to global competition and changing regulations.\",\n",
    "        },\n",
    "        {\n",
    "            \"Company Name\": original_company_name,\n",
    "            \"Strength\": f\"{original_company_name} has a diversified product portfolio.\",\n",
    "            \"Weakness\": f\"{original_company_name} may experience slow innovation cycles.\",\n",
    "            \"Opportunity\": f\"{original_company_name} can invest in artificial intelligence and automation.\",\n",
    "            \"Threat\": f\"{original_company_name} may be impacted by economic fluctuations or supply chain issues.\",\n",
    "        }\n",
    "    ]\n",
    "    return fallback_rows\n",
    "\n",
    "# --- Get latest file in folder ---\n",
    "def get_latest_file(folder, ext=\".xlsx\"):\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(ext)]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No Excel files found in folder.\")\n",
    "    files = sorted(files, key=lambda f: os.path.getmtime(os.path.join(folder, f)), reverse=True)\n",
    "    return os.path.join(folder, files[0])\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    model = configure_ai_model()\n",
    "    if not model:\n",
    "        return\n",
    "\n",
    "    # Step 1: Get latest Excel file\n",
    "    input_file = get_latest_file(INPUT_FOLDER)\n",
    "    print(f\"\\n📥 Reading file: {input_file}\")\n",
    "\n",
    "    # Step 2: Read company names\n",
    "    df = pd.read_excel(input_file, sheet_name=INPUT_SHEET)\n",
    "    companies = df[COMPANY_COL].dropna().unique()\n",
    "    print(f\"📊 Found {len(companies)} companies\")\n",
    "\n",
    "    # Step 3: Generate SWOT for each company\n",
    "    all_rows = []\n",
    "    for company in companies:\n",
    "        print(f\"🔍 Generating SWOT for: {company}\")\n",
    "        swot = generate_swot(model, original_company_name=company)\n",
    "        all_rows.extend(swot)  # always returns 2 valid rows\n",
    "        time.sleep(2)  # delay to avoid hitting API too fast\n",
    "\n",
    "    # Step 4: Save to Excel\n",
    "    output_df = pd.DataFrame(all_rows)\n",
    "    output_df.to_excel(OUTPUT_FILE, index=False, sheet_name=\"SWOT Analysis\")\n",
    "    print(f\"\\n✅ SWOT analysis saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "42d00829-3741-4f8c-8737-8db00c36ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligent virtual assistant\n"
     ]
    }
   ],
   "source": [
    "print(MARKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8bd899c9-6e57-4f1c-82b5-e18a6eb01190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI content for: intelligent virtual assistant\n",
      "DEBUG: Paragraph count = 7\n",
      "DEBUG: Word count = 855\n",
      "\n",
      "✅ Content saved successfully to: C:\\Users\\Madhan\\AI Content\\Impact of AI in global market.docx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import google.generativeai as genai\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "#API_KEY = \"AIzaSyBneAP9zHGsalKlex7cohxVTs9LLAgdRt8\"\n",
    "SAVE_PATH = r\"C:\\Users\\Madhan\\AI Content\\Impact of AI in global market\"\n",
    "\n",
    "OPENING_SENTENCE = \"\"\n",
    "\n",
    "# --- AI Setup ---\n",
    "def configure_ai_model():\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        return genai.GenerativeModel('gemini-2.5-pro')\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not configure AI. {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Clean text ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,\\n-]\", \"\", text)  # Remove special chars, keep punctuation\n",
    "    text = text.replace(\"’\", \"\").replace(\"‘\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "    text = text.replace(\"™\", \"\").replace(\"©\", \"\")\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)  # Collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# --- Validate content ---\n",
    "def validate_content(content):\n",
    "    paragraphs = [p.strip() for p in re.split(r'\\n{2,}', content) if p.strip()]\n",
    "    word_count = len(content.split())\n",
    "    required_phrase = f\"impact of artificial intelligence in the global {MARKET_NAME} market\".lower()\n",
    "\n",
    "    print(f\"DEBUG: Paragraph count = {len(paragraphs)}\")\n",
    "    print(f\"DEBUG: Word count = {word_count}\")\n",
    "\n",
    "    if len(paragraphs) < 6:\n",
    "        print(f\"Validation failed: Only {len(paragraphs)} paragraphs found.\")\n",
    "        return False\n",
    "\n",
    "    if word_count < 600:\n",
    "        print(f\"Validation failed: Only {word_count} words found.\")\n",
    "        return False\n",
    "\n",
    "    first_paragraph = paragraphs[0].lower()\n",
    "    last_paragraph = paragraphs[-1].lower()\n",
    "\n",
    "    if required_phrase not in first_paragraph:\n",
    "        print(\"Validation failed: Required phrase not found in first paragraph.\")\n",
    "        return False\n",
    "\n",
    "    if required_phrase not in last_paragraph:\n",
    "        print(\"Validation failed: Required phrase not found in last paragraph.\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# --- Generate content ---\n",
    "def generate_content(model):\n",
    "    prompt = f\"\"\"\n",
    "Generate at least six to seven clearly separated paragraphs of original content, totaling at least 600 words about the impact of artificial intelligence in the global {MARKET_NAME} market. \n",
    "Each paragraph must be separated by a blank line (two newline characters).\n",
    "The content must in the first (begin) paragraph with the exact phrase: The impact of artificial intelligence in the global {MARKET_NAME} market.\n",
    "This means the first and last paragraphs must include this phrase exactly as written.\n",
    "Use real examples of companies applying artificial intelligence in this market. Support analysis with numbers, comparisons, or outcomes.\n",
    "Avoid market size data. Expand all abbreviations on first use.\n",
    "Do not use serial numbers, quotation marks, &, apostrophes, em dashes, trademark symbols, or any commercial formatting words like vendor, bold, italic, underline.\n",
    "Do not use words such as vendor, bold, italic, underline, R and D, M and A, R & D, &, vendors.\n",
    "Make the text humanlike, insightful, and untraceable to artificial intelligence or market research sources.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        content = response.text.strip() if hasattr(response, \"text\") else str(response).strip()\n",
    "        full_text = f\"{OPENING_SENTENCE}\\n\\n{content}\"\n",
    "        cleaned = clean_text(full_text)\n",
    "        return cleaned\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Save to Word Document ---\n",
    "def save_to_word(content):\n",
    "    try:\n",
    "        title = f\"Impact of artificial intelligence in the global {MARKET_NAME} market\"\n",
    "        \n",
    "        doc = Document()\n",
    "        doc.add_heading(title, level=1)\n",
    "\n",
    "        paragraphs = [p.strip() for p in re.split(r'\\n{2,}', content) if p.strip()]\n",
    "        for para in paragraphs:\n",
    "            doc.add_paragraph(para)\n",
    "\n",
    "        # Ensure folder exists\n",
    "        os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "\n",
    "        file_path = SAVE_PATH if SAVE_PATH.lower().endswith('.docx') else SAVE_PATH + \".docx\"\n",
    "        doc.save(file_path)\n",
    "        print(f\"\\n✅ Content saved successfully to: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Word file: {e}\")\n",
    "\n",
    "# --- Main ---\n",
    "def main():\n",
    "    model = configure_ai_model()\n",
    "    if not model:\n",
    "        return\n",
    "\n",
    "    print(f\"Generating AI content for: {MARKET_NAME}\")\n",
    "    max_retries = 5\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        content = generate_content(model)\n",
    "        if content and validate_content(content):\n",
    "            save_to_word(content)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Retrying... attempt {attempt + 1} of {max_retries}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to generate valid content after maximum retries.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "153cd0b2-a888-499b-abc1-18b4da3feb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file saved as: C:\\Users\\Madhan\\AI Content\\Impact_of_AI_in_global_market.xlsx\n",
      "Title extracted: Impact of artificial intelligence in the global intelligent virtual assistant market\n",
      "Content length: 6087 characters\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Path to the Word file\n",
    "word_file_path = r\"C:\\Users\\Madhan\\AI Content\\Impact of AI in global market.docx\"\n",
    "\n",
    "# Load the Word document\n",
    "doc = Document(word_file_path)\n",
    "\n",
    "# Extract title and content\n",
    "title = None\n",
    "content_paragraphs = []\n",
    "\n",
    "for para in doc.paragraphs:\n",
    "    if para.style.name == 'Heading 1' and not title:\n",
    "        title = para.text.strip()\n",
    "    elif para.text.strip():\n",
    "        content_paragraphs.append(para.text.strip())\n",
    "\n",
    "content_text = \"\\n\\n\".join(content_paragraphs)\n",
    "\n",
    "# Create Excel workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Headers\n",
    "ws['A1'] = \"Write Here\"\n",
    "ws['B1'] = \"Qualitative Analysis Title\"\n",
    "\n",
    "# Fill data\n",
    "ws['A2'] = content_text\n",
    "ws['B2'] = title if title else \"Qualitative Analysis Title\"\n",
    "\n",
    "# ✅ Save Excel file WITH .xlsx extension\n",
    "excel_file_path = r\"C:\\Users\\Madhan\\AI Content\\Impact_of_AI_in_global_market.xlsx\"\n",
    "wb.save(excel_file_path)\n",
    "\n",
    "# Console output\n",
    "print(f\"✅ Excel file saved as: {excel_file_path}\")\n",
    "print(f\"Title extracted: {title}\")\n",
    "print(f\"Content length: {len(content_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aea8a3b0-d762-407d-95ba-8aa8d395ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers found: ['Write Here', 'Qualitative Analysis Title']\n",
      "Original value in 'Write Here' A2: The impact of artificial intelligence in the global intelligent virtual assistant market is nothing short of foundational. Artificial intelligence, or AI, serves as the cognitive engine that elevates these assistants from simple, command-based programs into sophisticated conversational partners. Without the advancements in subsets of AI like machine learning and Natural Language Processing, the intelligent virtual assistants we interact with today would be incapable of understanding context, learning user preferences, or improving their performance over time. It is AI that enables the transition from a rigid, programmatic interaction to a fluid, intuitive dialogue, effectively creating the intelligence in intelligent virtual assistants. This core technology allows the system to parse complex human speech, discern intent from ambiguous phrasing, and access vast databases to provide relevant, timely answers.\n",
      "\n",
      "The evolution from early digital assistants to modern intelligent virtual assistants clearly illustrates the transformative power of AI. Initial iterations of this technology were often frustratingly literal, failing to comprehend anything outside of a narrow set of pre-programmed commands. They struggled with regional accents, colloquialisms, and the natural flow of human conversation. Today, thanks to deep learning models trained on immense datasets of language and sound, assistants are remarkably adept at interpretation. They can differentiate between homophones based on context, understand follow up questions without needing the subject to be repeated, and even adapt to the unique speech patterns of individual users. This leap in capability is a direct result of AI algorithms that continuously refine their understanding with every interaction, creating a more personalized and effective user experience.\n",
      "\n",
      "Major technology companies provide clear examples of this AI-driven enhancement in action. Google Assistant, for instance, leverages the companys colossal knowledge graph and search capabilities to provide deeply contextual answers. Its ability to maintain a conversational thread over several interactions, remembering previous questions to inform current answers, is a hallmark of advanced AI. Similarly, Amazons Alexa has utilized AI to build a massive and thriving ecosystem. The AI not only processes voice commands to play music or order products but also enables third party developers to create thousands of specialized skills, turning Alexa into a versatile hub for controlling smart homes and accessing diverse services. Apples Siri, a pioneer in the space, continually uses AI to improve its proactivity and integration across the companys hardware, suggesting actions based on a users habits and calendar events, demonstrating a more predictive and helpful form of assistance.\n",
      "\n",
      "Beyond the consumer sphere, AI-powered virtual assistants are reshaping enterprise operations, particularly in customer service and internal support. Companies are deploying sophisticated chatbots and voice assistants to handle a high volume of routine customer inquiries, freeing human agents to focus on more complex, high value issues. For example, some major financial institutions report that their AI-powered assistants successfully resolve over 70 percent of initial customer queries without human intervention. This not only leads to significant operational cost savings but also improves customer satisfaction by providing instant, 247 support. These enterprise assistants use AI to understand industry specific terminology, access customer relationship management systems, and guide users through troubleshooting processes, acting as efficient and knowledgeable first responders.\n",
      "\n",
      "One of the most profound impacts of artificial intelligence is the shift of virtual assistants from reactive tools to proactive partners. Instead of merely waiting for a command, advanced assistants now anticipate user needs. An assistant might proactively alert a user about heavy traffic on their route to a scheduled meeting, suggest reordering a frequently purchased item, or remind them to call a family member on their birthday based on past communication patterns. This proactivity is fueled by machine learning algorithms that analyze vast streams of personal datacalendars, location, communication history, and app usageto identify patterns and predict future needs. This capability fundamentally changes the relationship between the human and the machine, transforming the assistant into a genuinely helpful entity that simplifies and enhances daily life.\n",
      "\n",
      "Despite these remarkable advancements, the continued integration of AI into virtual assistants presents ongoing challenges and ethical considerations. The very nature of a learning assistant requires access to extensive personal data, raising significant privacy and security concerns for users. Furthermore, the potential for inherent bias within AI algorithms, learned from skewed data, could lead to unfair or inaccurate outcomes. Ensuring that these systems are transparent, secure, and equitable is a critical task for developers and policymakers. The future will likely see AI research focused on achieving a deeper, more nuanced understanding of human emotion and intent, leading to assistants that are not just intelligent, but also empathetic and socially aware.\n",
      "\n",
      "Ultimately, the entire trajectory and value proposition of the modern virtual assistant are inextricably linked to the progress of artificial intelligence. From understanding the subtleties of human language to anticipating our needs before we voice them, AI is the technology that has converted a futuristic concept into a practical and widely adopted tool. The continuous improvement of these assistants, their expanding role in our homes and workplaces, and their growing integration into the fabric of our digital lives all serve to highlight a single, undeniable fact. This profound transformation underscores the immense and ongoing impact of artificial intelligence in the global intelligent virtual assistant market.\n",
      "Converted HTML:\n",
      "<p>The impact of artificial intelligence in the global intelligent virtual assistant market is nothing short of foundational. Artificial intelligence, or AI, serves as the cognitive engine that elevates these assistants from simple, command-based programs into sophisticated conversational partners. Without the advancements in subsets of AI like machine learning and Natural Language Processing, the intelligent virtual assistants we interact with today would be incapable of understanding context, learning user preferences, or improving their performance over time. It is AI that enables the transition from a rigid, programmatic interaction to a fluid, intuitive dialogue, effectively creating the intelligence in intelligent virtual assistants. This core technology allows the system to parse complex human speech, discern intent from ambiguous phrasing, and access vast databases to provide relevant, timely answers.</p>\n",
      "<p>The evolution from early digital assistants to modern intelligent virtual assistants clearly illustrates the transformative power of AI. Initial iterations of this technology were often frustratingly literal, failing to comprehend anything outside of a narrow set of pre-programmed commands. They struggled with regional accents, colloquialisms, and the natural flow of human conversation. Today, thanks to deep learning models trained on immense datasets of language and sound, assistants are remarkably adept at interpretation. They can differentiate between homophones based on context, understand follow up questions without needing the subject to be repeated, and even adapt to the unique speech patterns of individual users. This leap in capability is a direct result of AI algorithms that continuously refine their understanding with every interaction, creating a more personalized and effective user experience.</p>\n",
      "<p>Major technology companies provide clear examples of this AI-driven enhancement in action. Google Assistant, for instance, leverages the companys colossal knowledge graph and search capabilities to provide deeply contextual answers. Its ability to maintain a conversational thread over several interactions, remembering previous questions to inform current answers, is a hallmark of advanced AI. Similarly, Amazons Alexa has utilized AI to build a massive and thriving ecosystem. The AI not only processes voice commands to play music or order products but also enables third party developers to create thousands of specialized skills, turning Alexa into a versatile hub for controlling smart homes and accessing diverse services. Apples Siri, a pioneer in the space, continually uses AI to improve its proactivity and integration across the companys hardware, suggesting actions based on a users habits and calendar events, demonstrating a more predictive and helpful form of assistance.</p>\n",
      "<p>Beyond the consumer sphere, AI-powered virtual assistants are reshaping enterprise operations, particularly in customer service and internal support. Companies are deploying sophisticated chatbots and voice assistants to handle a high volume of routine customer inquiries, freeing human agents to focus on more complex, high value issues. For example, some major financial institutions report that their AI-powered assistants successfully resolve over 70 percent of initial customer queries without human intervention. This not only leads to significant operational cost savings but also improves customer satisfaction by providing instant, 247 support. These enterprise assistants use AI to understand industry specific terminology, access customer relationship management systems, and guide users through troubleshooting processes, acting as efficient and knowledgeable first responders.</p>\n",
      "<p>One of the most profound impacts of artificial intelligence is the shift of virtual assistants from reactive tools to proactive partners. Instead of merely waiting for a command, advanced assistants now anticipate user needs. An assistant might proactively alert a user about heavy traffic on their route to a scheduled meeting, suggest reordering a frequently purchased item, or remind them to call a family member on their birthday based on past communication patterns. This proactivity is fueled by machine learning algorithms that analyze vast streams of personal datacalendars, location, communication history, and app usageto identify patterns and predict future needs. This capability fundamentally changes the relationship between the human and the machine, transforming the assistant into a genuinely helpful entity that simplifies and enhances daily life.</p>\n",
      "<p>Despite these remarkable advancements, the continued integration of AI into virtual assistants presents ongoing challenges and ethical considerations. The very nature of a learning assistant requires access to extensive personal data, raising significant privacy and security concerns for users. Furthermore, the potential for inherent bias within AI algorithms, learned from skewed data, could lead to unfair or inaccurate outcomes. Ensuring that these systems are transparent, secure, and equitable is a critical task for developers and policymakers. The future will likely see AI research focused on achieving a deeper, more nuanced understanding of human emotion and intent, leading to assistants that are not just intelligent, but also empathetic and socially aware.</p>\n",
      "<p>Ultimately, the entire trajectory and value proposition of the modern virtual assistant are inextricably linked to the progress of artificial intelligence. From understanding the subtleties of human language to anticipating our needs before we voice them, AI is the technology that has converted a futuristic concept into a practical and widely adopted tool. The continuous improvement of these assistants, their expanding role in our homes and workplaces, and their growing integration into the fabric of our digital lives all serve to highlight a single, undeniable fact. This profound transformation underscores the immense and ongoing impact of artificial intelligence in the global intelligent virtual assistant market.</p>\n",
      "✅ Updated cell A2 in 'Write Here' column with HTML content.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import html\n",
    "import openpyxl\n",
    "\n",
    "def convert_to_html(text):\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    html_lines = []\n",
    "    in_list = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith((\"-\", \"*\", \"•\")):\n",
    "            if not in_list:\n",
    "                html_lines.append(\"<ul>\")\n",
    "                in_list = True\n",
    "            html_lines.append(f\"<li>{html.escape(line[1:].strip())}</li>\")\n",
    "        else:\n",
    "            if in_list:\n",
    "                html_lines.append(\"</ul>\")\n",
    "                in_list = False\n",
    "            html_lines.append(f\"<p>{html.escape(line)}</p>\")\n",
    "    if in_list:\n",
    "        html_lines.append(\"</ul>\")\n",
    "\n",
    "    return \"\\n\".join(html_lines)\n",
    "\n",
    "def main():\n",
    "    file_path = r\"C:\\Users\\Madhan\\AI Content\\Impact_of_AI_in_global_market.xlsx\"  # <--- Replace with your exact filename!\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ File NOT found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    wb = openpyxl.load_workbook(file_path)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Print headers in first row to verify column names\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    print(f\"Headers found: {headers}\")\n",
    "\n",
    "    try:\n",
    "        col_idx = headers.index(\"Write Here\") + 1  # openpyxl columns start at 1\n",
    "    except ValueError:\n",
    "        print(\"❌ 'Write Here' column NOT found in the first row headers.\")\n",
    "        return\n",
    "\n",
    "    cell = ws.cell(row=2, column=col_idx)\n",
    "    print(f\"Original value in 'Write Here' A2: {cell.value}\")\n",
    "\n",
    "    if not cell.value:\n",
    "        print(\"⚠️ Cell A2 in 'Write Here' column is empty. Nothing to convert.\")\n",
    "        return\n",
    "\n",
    "    html_text = convert_to_html(str(cell.value))\n",
    "    print(f\"Converted HTML:\\n{html_text}\")\n",
    "\n",
    "    cell.value = html_text\n",
    "    wb.save(file_path)\n",
    "    print(\"✅ Updated cell A2 in 'Write Here' column with HTML content.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e29b51fb-1302-4a83-8f66-aa221199d978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the workbook\n",
    "source_file = r\"C:\\Users\\Madhan\\AI Content\\Impact_of_AI_in_global_market.xlsx\"\n",
    "wb = load_workbook(source_file)\n",
    "\n",
    "# Select the active sheet (or use wb['SheetName'] if you know the name)\n",
    "ws = wb.active\n",
    "\n",
    "# Apply headers\n",
    "ws['A1'] = \"Write Here\"\n",
    "ws['B1'] = \"Qualitative Analysis Title\"\n",
    "\n",
    "# Rename the sheet\n",
    "ws.title = \"Qualitative Analysis\"\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(source_file)\n",
    "\n",
    "print(\"Excel file updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "17598ca8-9290-421d-93b4-7dd728b7985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File copied successfully to: C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\\Global Intelligent Virtual Assistant Market 2025-2029_\\Global Intelligent Virtual Assistant Market 2025-2029_ AI Qualitative Analysis (no extension)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define output file location (no extension)\n",
    "OUTPUT_FILE = os.path.join(\n",
    "    r\"C:\\Users\\Madhan\\OneDrive - Infiniti Research\\Downloads\",\n",
    "    prefix,\n",
    "    f\"{prefix} AI Qualitative Analysis\"  # No .xlsx\n",
    ")\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# Define source file\n",
    "source_file = r\"C:\\Users\\Madhan\\AI Content\\Impact_of_AI_in_global_market.xlsx\"\n",
    "\n",
    "# Check if source file exists\n",
    "if not os.path.isfile(source_file):\n",
    "    print(f\"❌ Source file not found at: {source_file}\")\n",
    "else:\n",
    "    # Copy file to OUTPUT_FILE path (no extension)\n",
    "    shutil.copyfile(source_file, OUTPUT_FILE)\n",
    "    print(f\"✅ File copied successfully to: {OUTPUT_FILE} (no extension)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959c50f-dcfe-46f8-96cd-bb3c46b9f7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243eeda-514c-45dd-8833-c711f8dd492c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e07e3c-56de-47ec-96c0-073895010522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
