{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d1bd82-70ff-42d8-ab9c-607e492e613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: NC_Global Distributed AI Computing Market_Write1.docx\n",
      "Paragraph 2: ‚Äî\n",
      "Paragraph 3: √†, √®\n",
      "Overwritten: C:\\Users\\Madhan\\Write Final\\NC_Global Distributed AI Computing Market_Write1.docx\n",
      "--------------------------------------------------\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# Folder containing the Word file(s)\n",
    "folder_path = r\"C:\\Users\\Madhan\\Write Final\"\n",
    "\n",
    "# Expanded Italian accented character replacement map\n",
    "char_map = {\n",
    "    \"√†\": \"a\", \"√Ä\": \"A\",\n",
    "    \"√®\": \"e\", \"√à\": \"E\",\n",
    "    \"√©\": \"e\", \"√â\": \"E\",\n",
    "    \"√¨\": \"i\", \"√å\": \"I\",\n",
    "    \"√≤\": \"o\", \"√í\": \"O\",\n",
    "    \"√π\": \"u\", \"√ô\": \"U\",\n",
    "    \"‚Äî\":\", \"\n",
    "    \n",
    "}\n",
    "\n",
    "# Words that should remain untouched\n",
    "exceptional_words = {\"Œº, ¬µ\"}  # Add more as needed\n",
    "\n",
    "def clean_text(text, location=\"\", table_mode=False):\n",
    "    # Skip if text is exactly an exceptional word\n",
    "    if text.strip() in exceptional_words:\n",
    "        return text\n",
    "    \n",
    "    changes = []\n",
    "    \n",
    "    # Replace non-breaking space\n",
    "    if \"\\u00A0\" in text:\n",
    "        changes.append(\"non-breaking space\")\n",
    "        text = text.replace(\"\\u00A0\", \" \")\n",
    "    \n",
    "    # Replace Italian accented characters\n",
    "    for k, v in char_map.items():\n",
    "        if k in text:\n",
    "            changes.append(k)\n",
    "            text = text.replace(k, v)\n",
    "    \n",
    "    # Remove all other non-ASCII except bullet (‚Ä¢) and comma (,)\n",
    "    text, removed_chars = remove_unwanted_specials(text)\n",
    "    if removed_chars:\n",
    "        changes.extend(removed_chars)\n",
    "    \n",
    "    # Print changes\n",
    "    if changes:\n",
    "        print(f\"{location}: {', '.join(sorted(set(changes)))}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_unwanted_specials(text):\n",
    "    removed_chars = []\n",
    "    new_text = \"\"\n",
    "    for ch in text:\n",
    "        # Keep ASCII chars, bullet ‚Ä¢ (U+2022), and comma\n",
    "        if ch.isascii() or ch == \"‚Ä¢\" or ch == \",\":\n",
    "            new_text += ch\n",
    "        else:\n",
    "            removed_chars.append(ch)\n",
    "    return new_text, removed_chars\n",
    "\n",
    "# Process all .docx files\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.lower().endswith(\".docx\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        doc = Document(file_path)\n",
    "        \n",
    "        # Clean paragraphs\n",
    "        for i, para in enumerate(doc.paragraphs):\n",
    "            para.text = clean_text(para.text, f\"Paragraph {i+1}\")\n",
    "        \n",
    "        # Clean tables\n",
    "        for ti, table in enumerate(doc.tables):\n",
    "            for ri, row in enumerate(table.rows):\n",
    "                for ci, cell in enumerate(row.cells):\n",
    "                    cell.text = clean_text(cell.text, f\"Table {ti+1} Row {ri+1} Col {ci+1}\", table_mode=True)\n",
    "        \n",
    "        doc.save(file_path)\n",
    "        print(f\"Overwritten: {file_path}\\n{'-'*50}\")\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33103af-94fe-4a62-9df9-2385979c4473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Updated file: NC_Global Distributed AI Computing Market_Write1.docx\n",
      " - Replaced: \"  \" ‚Üí \" \"\n",
      " - Replaced: \"vendor\" ‚Üí \"company\"\n",
      " - Replaced: \"vendors\" ‚Üí \"companies\"\n",
      " - Replaced: \"Vendor\" ‚Üí \"Company\"\n",
      " - Replaced: \"Vendors\" ‚Üí \"Companies\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "# Folder containing Word documents\n",
    "folder_path = r\"C:\\Users\\Madhan\\Write Final\"\n",
    "\n",
    "# Replacement dictionary (keys will be treated as regex patterns)\n",
    "replacements = {\n",
    "    r\"\\bvendor\\b\": \"company\",\n",
    "    r\"\\bvendors\\b\": \"companies\",\n",
    "    r\"\\bVendor\\b\": \"Company\",\n",
    "    r\"\\bVendors\\b\": \"Companies\",\n",
    "    r\"\\bvendor\\.\\b\": \"company.\",\n",
    "    r\"\\bvendors\\.\\b\": \"companies.\",\n",
    "    r\"\\bVendor\\.\\b\": \"Company.\",\n",
    "    r\"\\bVendors\\.\\b\": \"Companies.\",\n",
    "    r\"‚Äî\": \", \",\n",
    "    r\" & \": \" and \",\n",
    "    r\"&\": \" and \",\n",
    "    r\"‚Äì\": \"-\",\n",
    "    r\"mergers and acquisitions\\s*\\(merger and acquisition\\)\": \"mergers and acquisitions\",\n",
    "    r\"\\(merger and acquisition\\)\\s*mergers and acquisitions\": \"mergers and acquisitions\",\n",
    "    r\"mergers and acquisitions\\(merger and acquisition\\)\": \"mergers and acquisitions\",\n",
    "    r\"\\(merger and acquisition\\)mergers and acquisitions\": \"mergers and acquisitions\",\n",
    "    r\"‚Äôs\": \"\",\n",
    "    r\"‚Äô\": \"\",\n",
    "    r\"'s\": \"\",\n",
    "    r\"'\": \"\",\n",
    "    r\"‚Äò\": \"\",\n",
    "    r\"‚Äú\":\"\",\n",
    "    r\"‚Äù\":\"\",\n",
    "    r\"For instance, In\":\"For instance, in\",\n",
    "    r\"  \":\" \",\n",
    "    r\"‚Çπ\":\"INR \",\n",
    "    r\"‚Ç¨\":\"EUR\",\n",
    "    r\"√¥\":\"o\",\n",
    "    r\"\\b(R&D)\\b\": \"research and development\",\n",
    "    r\"\\b(R and D)\\b\": \"research and development\",\n",
    "    r\"\\b(R & D)\\b\": \"research and development\",\n",
    "    r\"\\b(M and A)\\b\": \"mergers and acquisitions\",\n",
    "    r\"\\b(M&A)\\b\": \"mergers and acquisitions\",\n",
    "    r\"\\b(M & A)\\b\": \"mergers and acquisitions\",\n",
    "    r\"\\bR&D\\b\": \"Research and development\",\n",
    "    r\"\\bR and D\\b\": \"Research and development\",\n",
    "    r\"\\bR & D\\b\": \"Research and development\",\n",
    "    r\"\\bM and A\\b\": \"Mergers and acquisitions\",\n",
    "    r\"\\bM&A\\b\": \"Mergers and acquisitions\",\n",
    "    r\"\\bM & A\\b\": \"Mergers and acquisitions\",\n",
    "    r\"\\research and development (research and development)\": \"research and development\",\n",
    "     r\"research and development\\s*\\(research and development\\)\": \"research and development\",\n",
    "    r\"\\(research and development\\)\\s*research and development\": \"research and development\",\n",
    "    r\"research and development\\(research and development\\)\": \"research and development\",\n",
    "    r\"\\(research and development\\)research and development\": \"research and development\"\n",
    "}\n",
    "\n",
    "# Process all .docx files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".docx\") and not filename.startswith(\"~$\"):  # Skip temp files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        doc = Document(file_path)\n",
    "        changes = []\n",
    "\n",
    "        for para in doc.paragraphs:\n",
    "            for run in para.runs:\n",
    "                text = run.text\n",
    "                for pattern, replacement in replacements.items():\n",
    "                    matches = list(re.finditer(pattern, text))\n",
    "                    if matches:\n",
    "                        changes.extend([(match.group(), replacement) for match in matches])\n",
    "                        text = re.sub(pattern, replacement, text)\n",
    "                run.text = text\n",
    "\n",
    "        if changes:\n",
    "            doc.save(file_path)\n",
    "            print(f\"\\n‚úÖ Updated file: {filename}\")\n",
    "            # Print unique replacements made in this file\n",
    "            unique_changes = list(dict.fromkeys(changes))  # Remove duplicates while preserving order\n",
    "            for before, after in unique_changes:\n",
    "                print(f' - Replaced: \"{before}\" ‚Üí \"{after}\"')\n",
    "        else:\n",
    "            print(f\"\\nNo changes needed: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3b4fd0-c4b5-4e17-8a82-73301889f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Moved existing Excel file to HTML folder: Global Real-Time AI Agent Assist Market 2025-2029_Write_colored.xlsx\n",
      "‚úÖ Processed: NC_Global Distributed AI Computing Market_Write1_colored.xlsx\n",
      "\n",
      "üìä Summary:\n",
      "‚úÖ Successfully processed: 1\n",
      "‚ùå Failed to process: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import html\n",
    "import re\n",
    "import shutil\n",
    "import xlsxwriter\n",
    "from docx import Document\n",
    "\n",
    "# Keywords to be flagged in HTML content\n",
    "ERROR_KEYWORDS = [\n",
    "    \"‚Äî\", \"√¢\", \"vendor\", \"Vendor\", \"vendors\", \"Vendors\", \"&\", \"  \",\"   \", \"199\", \"2000\", \"2001\", \"‚Äú\",\"‚Äù\", \"Covid\",\"COVID\", \" war \",\"Russia\", \"Ukraine\",\n",
    "    \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \n",
    "    \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\",\n",
    "    \"R&D\", \"R and D\", \"M and A\", \"M&A\", \"R & D\", \"M & A\", \"&nbsp;\", \"&nbsp;&nbsp;\",\n",
    "    \"&lt;\", \"&gt;\", \"&amp;\", \"&quot;\", \"&#39;\", \"&ensp;\", \"&emsp;\",\"‚Äî\",\"‚Äì\",\"‚Äî\",\"√†\", \"√®\", \"√©\", \"INR\", \"EUR \", \"‚Çπ\",\"¬Æ\", \"concentrated\", \"concentrate\",\n",
    "    \"√¨\", \"√≤\", \"√π\", \"\\\\,\", \"'\", \"¬©\", \"¬Æ\", \"‚Ñ¢\", \"‚Ç¨\", \"¬£\", \"¬•\",\"bold\", \"italic\",\"√ñ\"\n",
    "    \"¬ß\", \"¬∂\", \"‚Ä¢\", \"¬¢\", \"¬¨\", \"¬µ\", \"¬∞\", \"¬±\", \"√ó\", \"√∑\", \"‚Ä∞\",\n",
    "    \"√Ä\", \"√à\", \"√â\", \"√å\", \"√í\", \"√ô\", \"'s\",\"'\", \"undefined\", \"Undefined\", \" .\", \"}\",\"{\",\"+\",\"illion\",\"√ò\", \"global market\", \"regional market\",\"region market\", \"USD\", \"dollar\",\"dollars\"\n",
    "]\n",
    "\n",
    "def convert_to_html(content):\n",
    "    \"\"\"\n",
    "    Convert content (only Description) into HTML format, handling lists and paragraphs.\n",
    "    \"\"\"\n",
    "    html_output = []\n",
    "    in_list = False\n",
    "\n",
    "    # Split content into paragraphs, ensuring we're handling it line by line\n",
    "    paragraphs = [line.strip() for line in content.splitlines() if line.strip()]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return \"<p>No content available</p>\"  # Handle empty content gracefully\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # Check if the line starts with a bullet point or dash (for list items)\n",
    "        if para.startswith((\"-\", \"‚Ä¢\", \"*\")):\n",
    "            if not in_list:\n",
    "                html_output.append(\"<ul>\")\n",
    "                in_list = True\n",
    "            html_output.append(f\"<li>{html.escape(para[1:].strip())}</li>\")\n",
    "        else:\n",
    "            if in_list:\n",
    "                html_output.append(\"</ul>\")\n",
    "                in_list = False\n",
    "            # Convert normal paragraph text\n",
    "            html_output.append(f\"<p>{html.escape(para)}</p>\")\n",
    "    \n",
    "    if in_list:\n",
    "        html_output.append(\"</ul>\")\n",
    "    \n",
    "    return \"\\n\".join(html_output)\n",
    "\n",
    "def find_and_highlight_errors(text, workbook):\n",
    "    \"\"\"\n",
    "    Finds error keywords in text and highlights them in red.\n",
    "    \"\"\"\n",
    "    # Store the rich parts (default and error-colored text)\n",
    "    rich_parts = []\n",
    "    matches = []\n",
    "\n",
    "    # Search for keywords and highlight them in red\n",
    "    for keyword in ERROR_KEYWORDS:\n",
    "        pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "        for match in pattern.finditer(text):\n",
    "            matches.append((match.start(), match.end(), match.group()))\n",
    "\n",
    "    # Sort matches by start position\n",
    "    matches.sort(key=lambda x: x[0])\n",
    "    last_pos = 0\n",
    "\n",
    "    # Loop through the matches and apply rich formatting\n",
    "    for start, end, word in matches:\n",
    "        if start > last_pos:\n",
    "            rich_parts.append(text[last_pos:start])  # Append normal text\n",
    "        rich_parts.append(workbook.add_format({'font_color': 'red'}))  # Add red color format for the error word\n",
    "        rich_parts.append(text[start:end])  # Append the error word\n",
    "        last_pos = end\n",
    "\n",
    "    if last_pos < len(text):\n",
    "        rich_parts.append(text[last_pos:])  # Append the remaining text\n",
    "    \n",
    "    return rich_parts\n",
    "\n",
    "def extract_docx_content(file_path):\n",
    "    doc = Document(file_path)\n",
    "    data = []\n",
    "    current_heading = \"\"\n",
    "    current_paragraphs = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        style = para.style.name\n",
    "        text = para.text.strip()\n",
    "\n",
    "        if style in [\"Heading 1\", \"Heading 2\"]:\n",
    "            if text == \"AI Content\":\n",
    "                break\n",
    "            if current_heading:\n",
    "                combined = \"\\n\".join(current_paragraphs).strip()\n",
    "                if combined:  # Only add if there's content\n",
    "                    data.append((current_heading, combined))\n",
    "                current_paragraphs = []\n",
    "            current_heading = text\n",
    "        else:\n",
    "            current_paragraphs.append(text)\n",
    "\n",
    "    if current_heading and current_paragraphs:\n",
    "        combined = \"\\n\".join(current_paragraphs).strip()\n",
    "        if combined:  # Only add if there's content\n",
    "            data.append((current_heading, combined))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_excel_with_partial_color(data, output_path):\n",
    "    workbook = xlsxwriter.Workbook(output_path)\n",
    "    worksheet = workbook.add_worksheet(\"Word Data\")\n",
    "\n",
    "    header_format = workbook.add_format({'bold': True})\n",
    "    normal_format = workbook.add_format({'font_color': 'black'})\n",
    "    red_format = workbook.add_format({'font_color': 'red'})  # Red formatting for error text\n",
    "\n",
    "    headers = [\"Title\", \"Description\", \"HTML tags\", \"Error\"]\n",
    "    for col_num, header in enumerate(headers):\n",
    "        worksheet.write(0, col_num, header, header_format)\n",
    "\n",
    "    for row_num, (header, content) in enumerate(data, start=1):\n",
    "        # Check for empty content\n",
    "        if not content:\n",
    "            print(f\"‚ö† Skipping row {row_num}: No content found for '{header}'\")\n",
    "            continue  # Skip if there's no content\n",
    "\n",
    "        # Write the Title (no conversion, plain text)\n",
    "        worksheet.write(row_num, 0, header)\n",
    "\n",
    "        # Write the Description (column B) as plain text\n",
    "        worksheet.write(row_num, 1, content)\n",
    "\n",
    "        # Convert the Description (column B) to HTML\n",
    "        html_content = convert_to_html(content)\n",
    "\n",
    "        # Write the HTML content (column C)\n",
    "        worksheet.write(row_num, 2, html_content)\n",
    "\n",
    "        # Find and highlight errors in the HTML content for the HTML tags column (column C)\n",
    "        highlighted_content = find_and_highlight_errors(html_content, workbook)\n",
    "\n",
    "        # Write the HTML content with highlighted errors in the HTML tags column (column C)\n",
    "        worksheet.write_rich_string(row_num, 2, *highlighted_content)\n",
    "\n",
    "        # Write the error report (if any errors found) in the Error column (column D)\n",
    "        found_errors = find_unwanted_keywords(html_content)\n",
    "        if found_errors:\n",
    "            error_msg = \"‚ö† Contains error-related text: \" + \", \".join(sorted(set(found_errors), key=str.lower))\n",
    "            worksheet.write(row_num, 3, error_msg)\n",
    "        else:\n",
    "            worksheet.write(row_num, 3, \"\", normal_format)\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "def find_unwanted_keywords(text):\n",
    "    \"\"\"\n",
    "    Return a list of unwanted keywords found in the text.\n",
    "    \"\"\"\n",
    "    return [word for word in ERROR_KEYWORDS if word.lower() in text.lower()]\n",
    "\n",
    "def move_existing_excels_to_html(folder_path):\n",
    "    html_folder = os.path.join(folder_path, \"HTML\")\n",
    "    os.makedirs(html_folder, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            src = os.path.join(folder_path, file)\n",
    "            dst = os.path.join(html_folder, file)\n",
    "            try:\n",
    "                shutil.move(src, dst)\n",
    "                print(f\"üìÅ Moved existing Excel file to HTML folder: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Failed to move {file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    folder_path = r\"C:\\Users\\Madhan\\Write Final\"  # Change as needed\n",
    "\n",
    "    # Step 1: Move existing .xlsx files before processing\n",
    "    move_existing_excels_to_html(folder_path)\n",
    "\n",
    "    # Step 2: Process .docx files and generate new .xlsx\n",
    "    processed, failed = 0, 0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Skip temporary files (e.g., those starting with ~$)\n",
    "        if filename.startswith(\"~$\"):\n",
    "            continue\n",
    "\n",
    "        if filename.endswith(\".docx\"):\n",
    "            docx_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                content_data = extract_docx_content(docx_path)\n",
    "                if content_data:  # Ensure there's content to process\n",
    "                    excel_filename = os.path.splitext(filename)[0] + \"_colored.xlsx\"\n",
    "                    excel_output_path = os.path.join(folder_path, excel_filename)\n",
    "                    save_to_excel_with_partial_color(content_data, excel_output_path)\n",
    "                    print(f\"‚úÖ Processed: {excel_filename}\")\n",
    "                    processed += 1\n",
    "                else:\n",
    "                    print(f\"‚ö† Skipping {filename}: No content found.\")\n",
    "                    failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to process {filename}: {e}\")\n",
    "                failed += 1\n",
    "\n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"‚úÖ Successfully processed: {processed}\")\n",
    "    print(f\"‚ùå Failed to process: {failed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984aaa2b-cdca-4f58-93f4-9e2a416696f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84eb80df-ecb7-4fc5-be44-71fb2b96c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "###finnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69411951-7dc8-4159-add5-0cb240b12361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1269395-6b53-4480-aca8-1c0afc407035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Matching files found for prefix 'Global High-Density Polyethylene (Hdpe)':\n",
      "‚Üí Write Output file: Global High-Density Polyethylene (HDPE) Market 2025-2029_Write Output1_converted.xlsx\n",
      "‚Üí Write Final file:  Global High-Density Polyethylene (HDPE) Market 2025-2029_Write_colored.xlsx\n",
      "\n",
      "Initial data shapes:\n",
      "Write Output1: (193, 3)\n",
      "Write Final: (17, 4)\n",
      "\n",
      "After dropping columns:\n",
      "Write Output1 shape: (193, 2)\n",
      "Write Final shape: (17, 2)\n",
      "\n",
      "After removing unwanted rows:\n",
      "Write Output1 shape: (193, 2)\n",
      "Write Final shape: (17, 2)\n",
      "\n",
      "Replaced 'Company Landscape' with 'Vendor Landscape' in Write Final Unique IDs\n",
      "Original 'Company Landscape' count: 1\n",
      "'Vendor Landscape' count before replacement: 0\n",
      "'Vendor Landscape' count after replacement: 1\n",
      "\n",
      "Unique IDs count (after filtering):\n",
      "Write Output1 unique IDs: 192\n",
      "Write Final unique IDs: 16\n",
      "Matched Unique IDs: 15\n",
      "‚Üí Bargaining power of buyers\n",
      "‚Üí Bargaining power of suppliers\n",
      "‚Üí Market Challenges 1\n",
      "‚Üí Market Definition\n",
      "‚Üí Market Drivers 1\n",
      "‚Üí Market Trends 1\n",
      "‚Üí Notes and caveats\n",
      "‚Üí Objective\n",
      "‚Üí Region 1 - Year on Year Growth 2020 - 2025\n",
      "‚Üí Segment 1.1 Year on Year Growth 2020 - 2025\n",
      "‚Üí Segment 2.1 Year on Year Growth 2020 - 2025\n",
      "‚Üí Threat of new entrants\n",
      "‚Üí Threat of rivalry\n",
      "‚Üí Threat of substitutes\n",
      "‚Üí Vendor Landscape\n",
      "\n",
      "Non-matched Unique IDs in Write Final: 1\n",
      "‚Üí Segment 3.1 Year on Year Growth 2020 2025\n",
      "\n",
      "Final merged dataframe shape: (194, 3)\n",
      "\n",
      "‚úÖ Final merged file saved at:\n",
      "C:\\Users\\Madhan\\Write Final\\Write Upload\\Global_High-Density_Polyethylene_(Hdpe)Market_Upload.xlsx\n",
      "\n",
      "‚úÖ Final file copied without extension to:\n",
      "C:\\Users\\Madhan\\Write Final\\Write Final Upload\\Global_High-Density_Polyethylene_(Hdpe)Market_Upload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define folder paths\n",
    "write_files_folder = r\"C:\\Users\\Madhan\\Write Output\\Write Files\"\n",
    "write_final_folder = r\"C:\\Users\\Madhan\\Write Final\"\n",
    "output_save_folder = r\"C:\\Users\\Madhan\\Write Final\\Write Upload\"\n",
    "\n",
    "# Get Excel files\n",
    "write_files = [f for f in os.listdir(write_files_folder) if f.endswith(('.xlsx', '.xls'))]\n",
    "if not write_files:\n",
    "    raise FileNotFoundError(\"‚ùå No Excel files found in Write Files folder.\")\n",
    "\n",
    "write_final_files = [f for f in os.listdir(write_final_folder) if f.endswith(('.xlsx', '.xls'))]\n",
    "if not write_final_files:\n",
    "    raise FileNotFoundError(\"‚ùå No Excel files found in Write Final folder.\")\n",
    "\n",
    "# Function to extract prefix up to the word \"market\"\n",
    "def get_market_prefix(name):\n",
    "    name = name.lower()\n",
    "    index = name.find(\"market\")\n",
    "    if index != -1:\n",
    "        return name[:index].strip().replace(\" \", \"_\")\n",
    "    return None\n",
    "\n",
    "# Build mapping of prefixes\n",
    "write_file_map = {get_market_prefix(f): f for f in write_files if get_market_prefix(f)}\n",
    "write_final_map = {get_market_prefix(f): f for f in write_final_files if get_market_prefix(f)}\n",
    "\n",
    "# Match based on shared prefix\n",
    "matched_prefix = None\n",
    "for prefix in write_file_map:\n",
    "    if prefix in write_final_map:\n",
    "        matched_prefix = prefix\n",
    "        break\n",
    "\n",
    "if not matched_prefix:\n",
    "    raise FileNotFoundError(\"‚ùå No matching files based on 'Market' prefix found.\")\n",
    "\n",
    "matched_write_file = write_file_map[matched_prefix]\n",
    "matched_final_file = write_final_map[matched_prefix]\n",
    "\n",
    "print(f\"\\n‚úÖ Matching files found for prefix '{matched_prefix.replace('_', ' ').title()}':\")\n",
    "print(f\"‚Üí Write Output file: {matched_write_file}\")\n",
    "print(f\"‚Üí Write Final file:  {matched_final_file}\")\n",
    "\n",
    "# File paths\n",
    "write_output1_path = os.path.join(write_files_folder, matched_write_file)\n",
    "write_final_path = os.path.join(write_final_folder, matched_final_file)\n",
    "\n",
    "# Read Excel files (skip first row)\n",
    "df_output1 = pd.read_excel(write_output1_path, header=None, skiprows=1)\n",
    "df_final = pd.read_excel(write_final_path, header=None, skiprows=1)\n",
    "\n",
    "print(f\"\\nInitial data shapes:\\nWrite Output1: {df_output1.shape}\\nWrite Final: {df_final.shape}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_final.drop(columns=[col for col in [1, 3] if col in df_final.columns], inplace=True)\n",
    "if 2 in df_output1.columns:\n",
    "    df_output1.drop(columns=[2], inplace=True)\n",
    "\n",
    "print(f\"\\nAfter dropping columns:\\nWrite Output1 shape: {df_output1.shape}\\nWrite Final shape: {df_final.shape}\")\n",
    "\n",
    "# Filter rows\n",
    "keep_starts = [\n",
    "    \"Market Drivers 1\", \"Market Challenges 1\", \"Market Trends 1\",\n",
    "    \"Market Drivers 2\", \"Market Challenges 2\", \"Market Trends 2\",\n",
    "    \"Market Drivers 3\", \"Market Challenges 3\", \"Market Trends 3\",\n",
    "    \"Market Definition\"\n",
    "]\n",
    "remove_starts = [\"Five Force Analysis\", \"Market Segmentation by\"]\n",
    "\n",
    "def filter_rows(df, keep_starts, remove_starts):\n",
    "    mask_keep = df[0].astype(str).str.startswith(tuple(keep_starts))\n",
    "    mask_remove = ~df[0].astype(str).str.startswith(tuple(remove_starts))\n",
    "    return df[mask_keep | mask_remove]\n",
    "\n",
    "df_output1 = filter_rows(df_output1, keep_starts, remove_starts)\n",
    "df_final = filter_rows(df_final, keep_starts, remove_starts)\n",
    "\n",
    "print(f\"\\nAfter removing unwanted rows:\\nWrite Output1 shape: {df_output1.shape}\\nWrite Final shape: {df_final.shape}\")\n",
    "\n",
    "# Replace \"Company Landscape\" with \"Vendor Landscape\"\n",
    "company_count = df_final[0].astype(str).str.contains(\"Company Landscape\").sum()\n",
    "vendor_before = df_final[0].astype(str).str.contains(\"Vendor Landscape\").sum()\n",
    "\n",
    "df_final[0] = df_final[0].replace(\"Company Landscape\", \"Vendor Landscape\")\n",
    "\n",
    "vendor_after = df_final[0].astype(str).str.contains(\"Vendor Landscape\").sum()\n",
    "print(f\"\\nReplaced 'Company Landscape' with 'Vendor Landscape' in Write Final Unique IDs\")\n",
    "print(f\"Original 'Company Landscape' count: {company_count}\")\n",
    "print(f\"'Vendor Landscape' count before replacement: {vendor_before}\")\n",
    "print(f\"'Vendor Landscape' count after replacement: {vendor_after}\")\n",
    "\n",
    "# Ensure column 0 is string\n",
    "df_output1[0] = df_output1[0].astype(str)\n",
    "df_final[0] = df_final[0].astype(str)\n",
    "\n",
    "# Matched IDs\n",
    "matched_ids = set(df_output1[0]).intersection(set(df_final[0]))\n",
    "matched_ids_list = sorted(matched_ids)\n",
    "\n",
    "# Lookup dictionary\n",
    "if 2 not in df_final.columns:\n",
    "    raise KeyError(\"Expected column 2 not found after dropping.\")\n",
    "lookup_dict = df_final.drop_duplicates(subset=0).set_index(0)[2].to_dict()\n",
    "\n",
    "# Update Write Output1\n",
    "df_output1[1] = df_output1.apply(lambda row: lookup_dict.get(row[0], row[1]), axis=1)\n",
    "\n",
    "# Add non-matching rows from final\n",
    "non_matching_rows = df_final[~df_final[0].isin(df_output1[0])].copy()\n",
    "non_matching_rows[1] = non_matching_rows[2]\n",
    "non_matched_ids_list = sorted(non_matching_rows[0].unique())\n",
    "\n",
    "# Unique ID analysis\n",
    "output1_ids = df_output1[0].nunique()\n",
    "final_ids = df_final[0].nunique()\n",
    "\n",
    "# Final merge\n",
    "final_merged = pd.concat([df_output1, non_matching_rows], ignore_index=True)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nUnique IDs count (after filtering):\")\n",
    "print(f\"Write Output1 unique IDs: {output1_ids}\")\n",
    "print(f\"Write Final unique IDs: {final_ids}\")\n",
    "print(f\"Matched Unique IDs: {len(matched_ids_list)}\")\n",
    "for uid in matched_ids_list:\n",
    "    print(f\"‚Üí {uid}\")\n",
    "\n",
    "print(f\"\\nNon-matched Unique IDs in Write Final: {len(non_matched_ids_list)}\")\n",
    "for uid in non_matched_ids_list:\n",
    "    print(f\"‚Üí {uid}\")\n",
    "\n",
    "print(f\"\\nFinal merged dataframe shape: {final_merged.shape}\")\n",
    "\n",
    "# --- TRANSPOSE COLUMNS A AND B INTO ROW 1 AND ROW 2 ---\n",
    "row1 = final_merged[0].tolist()\n",
    "row2 = final_merged[1].tolist()\n",
    "\n",
    "# Replace all kinds of dashes \"‚Äì\" with \"-\" in Row 1\n",
    "row1 = [str(item).replace(\"‚Äì\", \"-\") for item in row1]\n",
    "\n",
    "# Handle duplicate \"Market Definition\" in Row 1\n",
    "def fix_market_definition_duplicates(items):\n",
    "    seen = 0\n",
    "    new_items = []\n",
    "    for item in items:\n",
    "        if item == \"Market Definition\":\n",
    "            seen += 1\n",
    "            if seen == 2:\n",
    "                # Change second occurrence to \" Market Definition\"\n",
    "                new_items.append(\" Market Definition\")\n",
    "                continue\n",
    "        new_items.append(item)\n",
    "    return new_items\n",
    "\n",
    "row1 = fix_market_definition_duplicates(row1)\n",
    "\n",
    "# Create transposed DataFrame\n",
    "transposed_df = pd.DataFrame([row1, row2])\n",
    "\n",
    "# Save output with sheet name \"data\"\n",
    "output_filename = f\"{matched_prefix.title()}Market_Upload.xlsx\"\n",
    "output_path = os.path.join(output_save_folder, output_filename)\n",
    "transposed_df.to_excel(output_path, index=False, header=False, sheet_name='data')\n",
    "\n",
    "print(f\"\\n‚úÖ Final merged file saved at:\\n{output_path}\")\n",
    "\n",
    "# ‚úÖ NEW STEP: Copy final file (without matching) to new location with no extension\n",
    "no_ext_folder = r\"C:\\Users\\Madhan\\Write Final\\Write Final Upload\"\n",
    "os.makedirs(no_ext_folder, exist_ok=True)\n",
    "\n",
    "# Create no-extension filename from final saved Excel\n",
    "no_ext_filename = os.path.splitext(output_filename)[0]  # remove .xlsx\n",
    "no_ext_path = os.path.join(no_ext_folder, no_ext_filename)\n",
    "\n",
    "shutil.copy2(output_path, no_ext_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Final file copied without extension to:\\n{no_ext_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95403046-fcbf-4337-98d7-28ebc25517fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115fb981-57ca-4c2a-a10a-32f86df3e677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf78ed-559d-4da1-979b-4d2e732b6008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcc282-d135-418d-bea7-f262fa84393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270568fd-efc2-4e91-afe8-ea556455c18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348113e2-ec76-4b6e-ae41-726d248f8dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
